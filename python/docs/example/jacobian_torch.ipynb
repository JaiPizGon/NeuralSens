{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuralsens.partial_derivatives as ns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic dataset to check behavior of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100000\n",
    "n_columns = 8\n",
    "sm = np.random.normal(size=(samples,n_columns))\n",
    "df = pd.DataFrame(sm, columns=['X' + str(x) for x in range(1,n_columns+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check behavior of Jacobian function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create output Y as linear function of inputs with some non-linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Y'] = - 0.8 * df.X1 + 0.5 * df.X2 ** 2 - df.X3 * df.X4 + 0.1 * np.random.normal(size=(samples,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP model using the data.frame created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create random 80/20 % split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Y'].to_numpy(), df['Y'], test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tch = torch.FloatTensor(X_train).requires_grad_(True).to(device)\n",
    "X_test_tch = torch.FloatTensor(X_test).requires_grad_(True).to(device)\n",
    "y_train_tch = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "y_test_tch = torch.FloatTensor(y_test.to_numpy()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MLP model torch class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Sequential):\n",
    "    def __init__(self, input_size:int, output_size:int = 1, hidden_size:list = [10]):\n",
    "        # Store layers to initiate sequential neural network\n",
    "        layers           = []\n",
    "        first = True\n",
    "        for idx, neurons in enumerate(hidden_size):\n",
    "            if first:\n",
    "                layers += [torch.nn.Linear(input_size, neurons)]\n",
    "                first = False\n",
    "            else:\n",
    "                layers += [torch.nn.Linear(hidden_size[idx-1], neurons)]\n",
    "            layers += [torch.nn.Sigmoid()]\n",
    "        layers += [torch.nn.Linear(hidden_size[idx-1], output_size)]\n",
    "        super(MLP, self).__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=n_columns, output_size=1, hidden_size=[15,15])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model performance before training\n",
    "model.eval()\n",
    "y_pred = model(X_test_tch)\n",
    "before_train = criterion(y_pred.squeeze().to(device), y_test_tch)\n",
    "print('Test loss before training' , before_train.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "epoch = 0\n",
    "loss = before_train\n",
    "path=[]\n",
    "while loss.item() > 0.05:\n",
    "    optimizer.zero_grad() # Reset the gradient\n",
    "    epoch += 1\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tch)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze().to(device), y_train_tch)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model performance after training\n",
    "model.eval()\n",
    "y_pred = model(X_test_tch)\n",
    "before_train = criterion(y_pred.squeeze().to(device), y_test_tch)\n",
    "print('Test loss after training' , before_train.item())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute jacobian function and check sensitivity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain parameters to perform jacobian\n",
    "X = pd.DataFrame(X_train, columns=df.columns[df.columns != 'Y'])\n",
    "y = pd.DataFrame(y_train, columns=['Y'])\n",
    "sens_end_layer = 'last'\n",
    "sens_end_input = False\n",
    "sens_origin_layer = 0\n",
    "sens_origin_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_torch = []\n",
    "bias_torch = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        wts_torch.append(param.detach().T.to(device))\n",
    "    if \"bias\" in name:\n",
    "        bias_torch.append(param.detach().to(device))\n",
    "actfunc_torch = [\"identity\", \"logistic\", \"logistic\", \"identity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = ns.jacobian_mlp(wts_torch, bias_torch, actfunc_torch, X, y, use_torch=True, dev=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sensitivity metrics\n",
    "# For X1, mean should be around -0.8\n",
    "# For X2, X3, X4, std shall be much greater than their mean\n",
    "# For X5, mean and std shall be near 0\n",
    "jacobian.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neuralsens')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6293a8b2c86d9a52e3a722d726ae162a9dda2b649303347e21b2798b86287157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
