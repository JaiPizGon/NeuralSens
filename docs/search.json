[{"path":"https://jaipizgon.github.io/NeuralSens/articles/download.html","id":"neuralsens-application","dir":"Articles","previous_headings":"","what":"NeuralSens Application","title":"Download","text":"pleased announce availability application run NeuralSens library. application provides easy--use interface leveraging powerful features NeuralSens package. development application made possible thanks support Cátedra Santalucía Analytics Education.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/articles/download.html","id":"download-the-installer","dir":"Articles","previous_headings":"NeuralSens Application","what":"Download the Installer","title":"Download","text":"Choose appropriate installer operating system:","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/articles/download.html","id":"windows","dir":"Articles","previous_headings":"NeuralSens Application > Download the Installer","what":"Windows","title":"Download","text":"Windows installer","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/articles/download.html","id":"mac","dir":"Articles","previous_headings":"NeuralSens Application > Download the Installer","what":"Mac","title":"Download","text":"Mac Installer","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"José Portela González. Author. Antonio Muñoz San Roque. Author. Jaime Pizarroso Gonzalo. Author, contributor, maintainer.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Pizarroso J, Portela J, Muñoz (2022). “NeuralSens: Sensitivity Analysis Neural Networks.” Journal Statistical Software, 102(7), 1–36. doi:10.18637/jss.v102.i07.","code":"@Article{,   title = {{NeuralSens}: Sensitivity Analysis of Neural Networks},   author = {Jaime Pizarroso and Jos\\'e Portela and Antonio Mu\\~noz},   journal = {Journal of Statistical Software},   year = {2022},   volume = {102},   number = {7},   pages = {1--36},   doi = {10.18637/jss.v102.i07}, }"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"josé-portela-gonzález-joseportelaiitcomillasedu","dir":"","previous_headings":"","what":"José Portela González, jose.portela@iit.comillas.edu","title":"Sensitivity Analysis of Neural Networks","text":"development repository NeuralSens package. Functions within package can used analysis neural network models created R. development version package can installed Github: last version can installed CRAN:","code":"install.packages('devtools') library(devtools) install_github('JaiPizGon/NeuralSens/R') install.packages('NeuralSens')"},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"bug-reports","dir":"","previous_headings":"","what":"Bug reports","title":"Sensitivity Analysis of Neural Networks","text":"Please submit bug reports (suggestions) using issues tab GitHub page.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"functions","dir":"","previous_headings":"","what":"Functions","title":"Sensitivity Analysis of Neural Networks","text":"One function available analyze sensitivity multilayer perceptron, evaluating variable importance plotting analysis results. sample dataset also provided use examples. function S3 methods developed neural networks following packages: nnet, neuralnet, RSNNS, caret, neural, h2o forecast. Numeric inputs describe model weights also acceptable. Start loading package sample dataset. SensAnalysisMLP function analyze sensitivity output input plots three graphics information analysis. calculate sensitivity calculates partial derivatives output inputs using training data. first plot shows information mean standard deviation sensitivity among training data: - mean different zero, means output depends input output changes input change. - mean nearly zero, means output depend input. standard deviation also near zero almost sure output depend variable training data partial derivative zero. - standard deviation different zero means output non-linear relation input partial derivative derivative output depends value input. - standard deviation nearly zero means output linear relation input partial derivative output depend value input. second plot gives absolute measure importance inputs, calculating sum squares partial derivatives output inputs. third plot density plot partial derivatives output inputs among training data, giving similar information first plot. Apart plot created SensAnalysisMLP function internal call SensitivityPlot, plots can obtained analyze neural network model. forecast problem, SensTimePlot function returns plot shows sensitivity output changes time data.  Also, detailed plot distribution variables can obtained SensFeaturePlot function. function returns scatter plot violin plot input variable, point represent sensitivity value sample dataset. color point depends value input corresponding sample.","code":"library(NeuralSens) data(DAILY_DEMAND_TR) # Scale the data DAILY_DEMAND_TR[,4] <- DAILY_DEMAND_TR[,4]/10 DAILY_DEMAND_TR[,2] <- DAILY_DEMAND_TR[,2]/100 # Parameters of the neural network hidden_neurons <- 5 iters <- 250 decay <- 0.1  # create neural network library(caret) ctrl_tune <- trainControl(method = \"boot\",                           savePredictions = FALSE,                           summaryFunction = defaultSummary) set.seed(150) #For replication mod <- caret::train(form = DEM~TEMP + WD,                     data = DAILY_DEMAND_TR,                     method = \"nnet\",                     linout = TRUE,                     tuneGrid = data.frame(size = hidden_neurons,                                           decay = decay),                     maxit = iters,                     preProcess = c(\"center\",\"scale\"),                     trControl = ctrl_tune,                     metric = \"RMSE\")  # Analysis of the neural network sens <- SensAnalysisMLP(mod) SensTimePlot(sens, fdata = DAILY_DEMAND_TR, facet = TRUE) SensFeaturePlot(sens, fdata = DAILY_DEMAND_TR)"},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Sensitivity Analysis of Neural Networks","text":"Please, cite NeuralSens publications use: Pizarroso J, Portela J, Muñoz (2022). “NeuralSens: Sensitivity Analysis Neural Networks.” Journal Statistical Software, 102(7), 1-36. doi: 10.18637/jss.v102.i07 (URL: https://doi.org/10.18637/jss.v102.i07).","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Sensitivity Analysis of Neural Networks","text":"package released public domain General Public License GPL.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/index.html","id":"association","dir":"","previous_headings":"","what":"Association","title":"Sensitivity Analysis of Neural Networks","text":"Package created Institute Research Technology (IIT), link homepage","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation function of neuron — ActFunc","title":"Activation function of neuron — ActFunc","text":"Evaluate activation function neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation function of neuron — ActFunc","text":"","code":"ActFunc(type = \"sigmoid\", ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation function of neuron — ActFunc","text":"type character name activation function ... extra arguments needed calculate functions","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation function of neuron — ActFunc","text":"numeric output neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Activation function of neuron — ActFunc","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ActFunc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Activation function of neuron — ActFunc","text":"","code":"# Return the sigmoid activation function of a neuron ActivationFunction <- ActFunc(\"sigmoid\") # Return the tanh activation function of a neuron ActivationFunction <- ActFunc(\"tanh\") # Return the activation function of several layers of neurons actfuncs <- c(\"linear\",\"sigmoid\",\"linear\") ActivationFunctions <- sapply(actfuncs, ActFunc)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensAnalysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","title":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","text":"Obtain sensitivity alpha-curves associated MLP function obtained sensitivities returned SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensAnalysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","text":"","code":"AlphaSensAnalysis(   sens,   tol = NULL,   max_alpha = 15,   curve_equal_origin = FALSE,   inp_var = NULL,   line_width = 1,   title = \"Alpha curve of Lp norm values\",   alpha_bar = 1,   kind = \"line\" )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensAnalysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","text":"sens sensitivity object returned SensAnalysisMLP tol difference M_alpha maximum sensitivity sensitivity input variable max_alpha maximum alpha value analyze curve_equal_origin make curves begin (1,0) inp_var character indicating input variable show density plot. useful choosing plot_type='raw' show density plot one input variable. NULL, variables plotted density plot. default NULL. line_width int width line plot. title char title alpha-curves plot alpha_bar int alpha value show column plot. kind char select type plot: \"line\" \"bar\"","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensAnalysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","text":"alpha-curves MLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensAnalysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity alpha-curve associated to MLP function — AlphaSensAnalysis","text":"","code":"# \\donttest{ mod <- RSNNS::mlp(simdata[, c(\"X1\", \"X2\", \"X3\")], simdata[, \"Y\"],                  maxit = 1000, size = 15, linOut = TRUE)  sens <- SensAnalysisMLP(mod, trData = simdata,                         output_name = \"Y\", plot = FALSE)  AlphaSensAnalysis(sens)  # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","title":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","text":"Obtain sensitivity alpha-curve associated MLP function obtained sensitivities returned SensAnalysisMLP input variable.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","text":"","code":"AlphaSensCurve(sens, tol = NULL, max_alpha = 100)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","text":"sens raw sensitivities MLP output respect input variable. tol difference M_alpha maximum sensitivity sensitivity input variable max_alpha maximum alpha value analyze","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","text":"alpha-curve MLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/AlphaSensCurve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity alpha-curve associated to MLP function of an input variable — AlphaSensCurve","text":"","code":"# \\donttest{ mod <- RSNNS::mlp(simdata[, c(\"X1\", \"X2\", \"X3\")], simdata[, \"Y\"],                  maxit = 1000, size = 15, linOut = TRUE)  sens <- SensAnalysisMLP(mod, trData = simdata,                         output_name = \"Y\", plot = FALSE)  AlphaSensCurve(sens$raw_sens[[1]][,1]) #>   [1] 1.600859 2.024488 2.377516 2.683083 2.950296 3.184139 3.388838 3.568474 #>   [9] 3.726854 3.867346 3.992821 4.105667 4.207850 4.300979 4.386367 4.465087 #>  [17] 4.538020 4.605888 4.669291 4.728724 4.784603 4.837276 4.887040 4.934147 #>  [25] 4.978815 5.021234 5.061570 5.099970 5.136566 5.171475 5.204804 5.236652 #>  [33] 5.267107 5.296251 5.324160 5.350905 5.376551 5.401159 5.424785 5.447483 #>  [41] 5.469303 5.490290 5.510488 5.529939 5.548680 5.566747 5.584174 5.600993 #>  [49] 5.617234 5.632924 5.648091 5.662758 5.676950 5.690688 5.703993 5.716884 #>  [57] 5.729381 5.741501 5.753259 5.764673 5.775755 5.786521 5.796984 5.807156 #>  [65] 5.817048 5.826673 5.836040 5.845160 5.854042 5.862696 5.871130 5.879353 #>  [73] 5.887371 5.895194 5.902827 5.910277 5.917552 5.924657 5.931597 5.938380 #>  [81] 5.945009 5.951490 5.957828 5.964028 5.970094 5.976031 5.981842 5.987531 #>  [89] 5.993103 5.998561 6.003907 6.009147 6.014282 6.019317 6.024253 6.029094 #>  [97] 6.033842 6.038501 6.043072 6.047558 # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ChangeBootAlpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Change significance of boot SensMLP Class — ChangeBootAlpha","title":"Change significance of boot SensMLP Class — ChangeBootAlpha","text":"SensMLP Class object, change significance level statistical tests","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ChangeBootAlpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change significance of boot SensMLP Class — ChangeBootAlpha","text":"","code":"ChangeBootAlpha(x, boot.alpha)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ChangeBootAlpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change significance of boot SensMLP Class — ChangeBootAlpha","text":"x SensMLP object created SensAnalysisMLP boot.alpha float significance level","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ChangeBootAlpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Change significance of boot SensMLP Class — ChangeBootAlpha","text":"SensMLP object changed significance level. boot related metrics changed","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ChangeBootAlpha.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Change significance of boot SensMLP Class — ChangeBootAlpha","text":"","code":"# \\donttest{ ## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000   ## TRAIN nnet NNET --------------------------------------------------------  set.seed(150) nnetmod <- caret::train(DEM ~ .,                  data = fdata.Reg.tr,                  method = \"nnet\",                  tuneGrid = expand.grid(size = c(1), decay = c(0.01)),                  trControl = caret::trainControl(method=\"none\"),                  preProcess = c('center', 'scale'),                  linout = FALSE,                  trace = FALSE,                  maxit = 300) # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = fdata.Reg.tr,                                     plot = FALSE, boot.R=2, output_name='DEM') #> # Calculating bootstrap sensitivity measures #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100% NeuralSens::ChangeBootAlpha(sens, boot.alpha=0.1) #> Sensitivity analysis of 2-1-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $DEM #>             WD        TEMP #> [1,] 0.1657809 -0.06765913 #> [2,] 0.3102340 -0.12661386 #> [3,] 0.3185984 -0.13002759 #> [4,] 0.3180175 -0.12979052 #> [5,] 0.3204461 -0.13078166 # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/CombineSens.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity analysis plot over time of the data — CombineSens","title":"Sensitivity analysis plot over time of the data — CombineSens","text":"Plot sensitivity neural network output respect inputs time variable data provided","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/CombineSens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity analysis plot over time of the data — CombineSens","text":"","code":"CombineSens(object, comb_type = \"mean\")"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/CombineSens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity analysis plot over time of the data — CombineSens","text":"object SensMLP object generated SensAnalysisMLP several outputs (classification MLP) comb_type Function combine matrixes raw_sens component object. can \"mean\", \"median\" \"sqmean\". can also function combine rows matrixes","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/CombineSens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity analysis plot over time of the data — CombineSens","text":"SensMLP object sensitivities combined","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/CombineSens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity analysis plot over time of the data — CombineSens","text":"","code":"# \\donttest{ fdata <- iris ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata)[1:ncol(fdata)-1], collapse = \" + \") form <- formula(paste(names(fdata)[5], form, sep = \" ~ \"))  set.seed(150) mod <- nnet::nnet(form,                   data = fdata,                   linear.output = TRUE,                   size = hidden_neurons,                   decay = decay,                   maxit = iters) #> # weights:  43 #> initial  value 202.395197  #> iter  10 value 89.434352 #> iter  20 value 69.265889 #> iter  30 value 30.753230 #> iter  40 value 23.999431 #> iter  50 value 23.111929 #> iter  60 value 23.041209 #> iter  70 value 23.013926 #> iter  80 value 23.007422 #> final  value 23.007413  #> converged # mod should be a neural network classification model sens <- SensAnalysisMLP(mod, trData = fdata, output_name = 'Species') #> Loading required namespace: ggforce #> Warning: All aesthetics have length 1, but the data has 4 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range  combinesens <- CombineSens(sens, \"sqmean\") # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeHessMeasures.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot sensitivities of a neural network model — ComputeHessMeasures","title":"Plot sensitivities of a neural network model — ComputeHessMeasures","text":"Function plot sensitivities created SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeHessMeasures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot sensitivities of a neural network model — ComputeHessMeasures","text":"","code":"ComputeHessMeasures(sens)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeHessMeasures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot sensitivities of a neural network model — ComputeHessMeasures","text":"sens SensAnalysisMLP object created SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeHessMeasures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot sensitivities of a neural network model — ComputeHessMeasures","text":"SensAnalysisMLP object sensitivities calculated","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeHessMeasures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot sensitivities of a neural network model — ComputeHessMeasures","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot sensitivities of a neural network model — ComputeSensMeasures","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"Function plot sensitivities created SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"","code":"ComputeSensMeasures(sens)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"sens SensAnalysisMLP object created SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"SensAnalysisMLP object sensitivities calculated","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/ComputeSensMeasures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot sensitivities of a neural network model — ComputeSensMeasures","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TR.html","id":null,"dir":"Reference","previous_headings":"","what":"Data frame with 4 variables — DAILY_DEMAND_TR","title":"Data frame with 4 variables — DAILY_DEMAND_TR","text":"Training dataset values temperature working day predict electrical demand","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TR.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data frame with 4 variables — DAILY_DEMAND_TR","text":"data frame 1980 rows 4 variables: DATE date measure DEM electrical demand WD Working Day: index express much work made day TEMP weather temperature","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TR.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data frame with 4 variables — DAILY_DEMAND_TR","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TR.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Data frame with 4 variables — DAILY_DEMAND_TR","text":"Jose Portela Gonzalez","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TV.html","id":null,"dir":"Reference","previous_headings":"","what":"Data frame with 3 variables — DAILY_DEMAND_TV","title":"Data frame with 3 variables — DAILY_DEMAND_TV","text":"Validation dataset values temperature working day predict electrical demand","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TV.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data frame with 3 variables — DAILY_DEMAND_TV","text":"data frame 7 rows 3 variables: DATE date measure WD Working Day: index express much work made day TEMP weather temperature","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TV.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data frame with 3 variables — DAILY_DEMAND_TV","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DAILY_DEMAND_TV.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Data frame with 3 variables — DAILY_DEMAND_TV","text":"Jose Portela Gonzalez","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der2ActFunc.html","id":null,"dir":"Reference","previous_headings":"","what":"Second derivative of activation function of neuron — Der2ActFunc","title":"Second derivative of activation function of neuron — Der2ActFunc","text":"Evaluate second derivative activation function neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der2ActFunc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Second derivative of activation function of neuron — Der2ActFunc","text":"","code":"Der2ActFunc(type = \"sigmoid\", ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der2ActFunc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Second derivative of activation function of neuron — Der2ActFunc","text":"type character name activation function ... extra arguments needed calculate functions","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der2ActFunc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Second derivative of activation function of neuron — Der2ActFunc","text":"numeric output neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der2ActFunc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Second derivative of activation function of neuron — Der2ActFunc","text":"","code":"# Return derivative of the sigmoid activation function of a neuron ActivationFunction <- Der2ActFunc(\"sigmoid\") # Return derivative of the tanh activation function of a neuron ActivationFunction <- Der2ActFunc(\"tanh\") # Return derivative of the activation function of several layers of neurons actfuncs <- c(\"linear\",\"sigmoid\",\"linear\") ActivationFunctions <- sapply(actfuncs, Der2ActFunc)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der3ActFunc.html","id":null,"dir":"Reference","previous_headings":"","what":"Third derivative of activation function of neuron — Der3ActFunc","title":"Third derivative of activation function of neuron — Der3ActFunc","text":"Evaluate third derivative activation function neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der3ActFunc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Third derivative of activation function of neuron — Der3ActFunc","text":"","code":"Der3ActFunc(type = \"sigmoid\", ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der3ActFunc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Third derivative of activation function of neuron — Der3ActFunc","text":"type character name activation function ... extra arguments needed calculate functions","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der3ActFunc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Third derivative of activation function of neuron — Der3ActFunc","text":"numeric output neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/Der3ActFunc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Third derivative of activation function of neuron — Der3ActFunc","text":"","code":"# Return derivative of the sigmoid activation function of a neuron ActivationFunction <- Der3ActFunc(\"sigmoid\") # Return derivative of the tanh activation function of a neuron ActivationFunction <- Der3ActFunc(\"tanh\") # Return derivative of the activation function of several layers of neurons actfuncs <- c(\"linear\",\"sigmoid\",\"linear\") ActivationFunctions <- sapply(actfuncs, Der3ActFunc)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":null,"dir":"Reference","previous_headings":"","what":"Derivative of activation function of neuron — DerActFunc","title":"Derivative of activation function of neuron — DerActFunc","text":"Evaluate derivative activation function neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Derivative of activation function of neuron — DerActFunc","text":"","code":"DerActFunc(type = \"sigmoid\", ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Derivative of activation function of neuron — DerActFunc","text":"type character name activation function ... extra arguments needed calculate functions","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Derivative of activation function of neuron — DerActFunc","text":"numeric output neuron","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Derivative of activation function of neuron — DerActFunc","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/DerActFunc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Derivative of activation function of neuron — DerActFunc","text":"","code":"# Return derivative of the sigmoid activation function of a neuron ActivationFunction <- DerActFunc(\"sigmoid\") # Return derivative of the tanh activation function of a neuron ActivationFunction <- DerActFunc(\"tanh\") # Return derivative of the activation function of several layers of neurons actfuncs <- c(\"linear\",\"sigmoid\",\"linear\") ActivationFunctions <- sapply(actfuncs, DerActFunc)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":null,"dir":"Reference","previous_headings":"","what":"Define function to change the diagonal of array — diag3Darray<-","title":"Define function to change the diagonal of array — diag3Darray<-","text":"Define function change diagonal array","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define function to change the diagonal of array — diag3Darray<-","text":"","code":"diag3Darray(x) <- value"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define function to change the diagonal of array — diag3Darray<-","text":"x 3D array whose diagonal must c hanged value vector defining new values diagonal.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define function to change the diagonal of array — diag3Darray<-","text":"array elements zero except diagonal, dimensions c(dim,dim,dim)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define function to change the diagonal of array — diag3Darray<-","text":"diagonal 3D array defined elements positions c(int,int,int), .e., three digits .","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray-set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define function to change the diagonal of array — diag3Darray<-","text":"","code":"x <- array(1, dim = c(3,3,3)) diag3Darray(x) <- c(2,2,2) x #> , , 1 #>  #>      [,1] [,2] [,3] #> [1,]    2    1    1 #> [2,]    1    1    1 #> [3,]    1    1    1 #>  #> , , 2 #>  #>      [,1] [,2] [,3] #> [1,]    1    1    1 #> [2,]    1    2    1 #> [3,]    1    1    1 #>  #> , , 3 #>  #>      [,1] [,2] [,3] #> [1,]    1    1    1 #> [2,]    1    1    1 #> [3,]    1    1    2 #>  #  , , 1 # #  [,1] [,2] [,3] #  [1,]    2    1    1 #  [2,]    1    1    1 #  [3,]    1    1    1 # #  , , 2 # #  [,1] [,2] [,3] #  [1,]    1    1    1 #  [2,]    1    2    1 #  [3,]    1    1    1 # #  , , 3 # #  [,1] [,2] [,3] #  [1,]    1    1    1 #  [2,]    1    1    1 #  [3,]    1    1    2"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":null,"dir":"Reference","previous_headings":"","what":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"Define function create 'diagonal' array get diagonal array","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"","code":"diag3Darray(x = 1, dim = length(x), out = \"vector\")"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"x number vector defining value diagonal 3D array dim integer defining length diagonal. Default length(x). length(x) != 1, dim must equal length(x). character specifying type diagonal return (\"vector\" \"matrix\"). See Details","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"array elements zero except diagonal, dimensions c(dim,dim,dim)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"diagonal 3D array defined elements positions c(int,int,int), .e., three digits . diagonal returned, specifies return \"vector\" elements position c(int,int,int), \"matrix\" elements position c(int,dim,int), .e., dim = 2 -> elements (1,1,1),(2,1,2),(3,1,3),(1,2,1),(2,2,2),(3,2,3),(3,1,3),(3,2,3),(3,3,3).","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag3Darray.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag3Darray","text":"","code":"x <- diag3Darray(c(1,4,6), dim = 3) x #> , , 1 #>  #>      [,1] [,2] [,3] #> [1,]    1    0    0 #> [2,]    0    0    0 #> [3,]    0    0    0 #>  #> , , 2 #>  #>      [,1] [,2] [,3] #> [1,]    0    0    0 #> [2,]    0    4    0 #> [3,]    0    0    0 #>  #> , , 3 #>  #>      [,1] [,2] [,3] #> [1,]    0    0    0 #> [2,]    0    0    0 #> [3,]    0    0    6 #>  # , , 1 # # [,1] [,2] [,3] # [1,]    1    0    0 # [2,]    0    0    0 # [3,]    0    0    0 # # , , 2 # # [,1] [,2] [,3] # [1,]    0    0    0 # [2,]    0    4    0 # [3,]    0    0    0 # # , , 3 # # [,1] [,2] [,3] # [1,]    0    0    0 # [2,]    0    0    0 # [3,]    0    0    6 diag3Darray(x) #> [1] 1 4 6 # 1, 4, 6"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":null,"dir":"Reference","previous_headings":"","what":"Define function to change the diagonal of array — diag4Darray<-","title":"Define function to change the diagonal of array — diag4Darray<-","text":"Define function change diagonal array","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define function to change the diagonal of array — diag4Darray<-","text":"","code":"diag4Darray(x) <- value"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define function to change the diagonal of array — diag4Darray<-","text":"x 3D array whose diagonal must c hanged value vector defining new values diagonal.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define function to change the diagonal of array — diag4Darray<-","text":"array elements zero except diagonal, dimensions c(dim,dim,dim)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define function to change the diagonal of array — diag4Darray<-","text":"diagonal 3D array defined elements positions c(int,int,int), .e., three digits .","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray-set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define function to change the diagonal of array — diag4Darray<-","text":"","code":"x <- array(1, dim = c(4,4,4,4)) diag4Darray(x) <- c(2,2,2,2) x #> , , 1, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    2    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 2, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 3, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 4, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 1, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 2, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    2    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 3, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 4, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 1, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 2, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 3, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    2    1 #> [4,]    1    1    1    1 #>  #> , , 4, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 1, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 2, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 3, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    1 #>  #> , , 4, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    1    1 #> [2,]    1    1    1    1 #> [3,]    1    1    1    1 #> [4,]    1    1    1    2 #>  # , , 1, 1 # #      [,1] [,2] [,3] [,4] # [1,]    2    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 2, 1 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 3, 1 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 4, 1 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 1, 2 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 2, 2 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    2    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 3, 2 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 4, 2 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 1, 3 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 2, 3 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 3, 3 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    2    1 # [4,]    1    1    1    1 # # , , 4, 3 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 1, 4 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 2, 4 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 3, 4 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    1 # # , , 4, 4 # #      [,1] [,2] [,3] [,4] # [1,]    1    1    1    1 # [2,]    1    1    1    1 # [3,]    1    1    1    1 # [4,]    1    1    1    2"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":null,"dir":"Reference","previous_headings":"","what":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"Define function create 'diagonal' array get diagonal array","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"","code":"diag4Darray(x = 1, dim = length(x))"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"x number vector defining value diagonal 4D array dim integer defining length diagonal. Default length(x). length(x) != 1, dim must equal length(x).","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"array elements zero except diagonal, dimensions c(dim,dim,dim)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"diagonal 4D array defined elements positions c(int,int,int,int), .e., four digits .","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/diag4Darray.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define function to create a 'diagonal' array or get the diagonal of an array — diag4Darray","text":"","code":"x <- diag4Darray(c(1,3,6,2), dim = 4) x #> , , 1, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    1    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 2, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 3, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 4, 1 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 1, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 2, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    3    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 3, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 4, 2 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 1, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 2, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 3, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    6    0 #> [4,]    0    0    0    0 #>  #> , , 4, 3 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 1, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 2, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 3, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    0 #>  #> , , 4, 4 #>  #>      [,1] [,2] [,3] [,4] #> [1,]    0    0    0    0 #> [2,]    0    0    0    0 #> [3,]    0    0    0    0 #> [4,]    0    0    0    2 #>  # , , 1, 1 # #      [,1] [,2] [,3] [,4] # [1,]    1    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 2, 1 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 3, 1 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 4, 1 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 1, 2 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 2, 2 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    3    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 3, 2 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 4, 2 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 1, 3 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 2, 3 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 3, 3 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    6    0 # [4,]    0    0    0    0 # # , , 4, 3 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 1, 4 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 2, 4 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 3, 4 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    0 # # , , 4, 4 # #      [,1] [,2] [,3] [,4] # [1,]    0    0    0    0 # [2,]    0    0    0    0 # [3,]    0    0    0    0 # [4,]    0    0    0    2 diag4Darray(x) #> [1] 1 3 6 2 # 1, 3, 6, 2"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/find_critical_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Critical Value — find_critical_value","title":"Find Critical Value — find_critical_value","text":"function finds smallest x probability random variable less equal x greater equal 1 - alpha. uses uniroot function find empirical cumulative distribution function (ECDF) crosses 1 - alpha.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/find_critical_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Critical Value — find_critical_value","text":"","code":"find_critical_value(ecdf_func, alpha)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/find_critical_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Critical Value — find_critical_value","text":"ecdf_func ECDF function representing distribution random variable. alpha numeric value specifying significance level.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/find_critical_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Critical Value — find_critical_value","text":"smallest x P(X <= x) >= 1 - alpha.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/find_critical_value.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Critical Value — find_critical_value","text":"","code":"data <- rnorm(100) ecdf_data <- ecdf(data) critical_val <- find_critical_value(ecdf_data, 0.05)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessDotPlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","title":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","text":"3D Plot second derivatives neural network output respect inputs. function use plotly instead ggplot2 achieve better visualization","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessDotPlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","text":"","code":"HessDotPlot(   object,   fdata = NULL,   input_vars = \"all\",   input_vars2 = \"all\",   output_vars = \"all\",   surface = FALSE,   grid = FALSE,   color = NULL,   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessDotPlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","text":"object fitted neural network model array containing raw second derivatives function HessianMLP fdata data.frame containing data evaluate second derivatives model. input_vars character vector variables create scatter plot x-axis. \"\", scatter plots created input variables fdata. input_vars2 character vector variables create scatter plot y-axis. \"\", scatter plots created input variables fdata. output_vars character vector variables create scatter plot. \"\", scatter plots created output variables fdata. surface logical TRUE, 3D surface created instead 3D scatter plot (combinations different inputs) grid logical. TRUE, plots created show together using arrangeGrob. work Windows platforms due bugs plotly library. color character specifying name numeric variable fdata color 3D scatter plot. ... arguments passed  HessianMLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessDotPlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","text":"list 3D geom_point plots inputs variables representing sensitivity output respect inputs","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessDotPlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Second derivatives 3D scatter or surface plot against input values — HessDotPlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessDotPlot NeuralSens::HessDotPlot(nnetmod, fdata = nntrData, surface = TRUE, color = \"WD\") #> Loading required namespace: plotly"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessFeaturePlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature sensitivity plot — HessFeaturePlot","title":"Feature sensitivity plot — HessFeaturePlot","text":"Show distribution sensitivities output geom_sina() plot color depends input values","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessFeaturePlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature sensitivity plot — HessFeaturePlot","text":"","code":"HessFeaturePlot(object, fdata = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessFeaturePlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature sensitivity plot — HessFeaturePlot","text":"object fitted neural network model array containing raw sensitivities function SensAnalysisMLP fdata data.frame containing data evaluate sensitivity model. needed raw sensitivities passed object ... arguments passed  SensAnalysisMLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessFeaturePlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature sensitivity plot — HessFeaturePlot","text":"list Feature sensitivity plot described https://www.r-bloggers.com/2019/03/-gentle-introduction--shap-values--r/","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessFeaturePlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature sensitivity plot — HessFeaturePlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP hess <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) NeuralSens::HessFeaturePlot(hess)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity of MLP models — HessianMLP","title":"Sensitivity of MLP models — HessianMLP","text":"Function evaluating sensitivities inputs   variables mlp model","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity of MLP models — HessianMLP","text":"","code":"HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for default HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc = NULL,   deractfunc = NULL,   der2actfunc = NULL,   preProc = NULL,   terms = NULL,   output_name = NULL,   ... )  # S3 method for train HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for H2OMultinomialModel HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for H2ORegressionModel HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for list HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc,   ... )  # S3 method for mlp HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nn HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nnet HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nnetar HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for numeric HessianMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc = NULL,   preProc = NULL,   terms = NULL,   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity of MLP models — HessianMLP","text":"MLP.fit fitted neural network model .returnSens DEPRECATED plot logical whether plot analysis. default TRUE. .rawSens DEPRECATED sens_origin_layer numeric specifies layer neurons respect derivative must calculated. input layer specified 1 (default). sens_end_layer numeric specifies layer neurons derivative calculated. may also 'last' specify output layer (default). sens_origin_input logical specifies derivative must calculated respect inputs (TRUE) output (FALSE) sens_origin_layer layer model. default TRUE. sens_end_input logical specifies derivative calculated output (FALSE) input (TRUE) sens_end_layer layer model. default FALSE. ... additional arguments passed methods trData data.frame containing data evaluate sensitivity model actfunc character vector indicating activation function neurons layer. deractfunc character vector indicating derivative activation function neurons layer. der2actfunc character vector indicating second derivative activation function neurons layer. preProc preProcess structure applied training data. See also preProcess terms function applied training data create factors. See also train output_name character name output variable order avoid changing name output variable trData '.outcome'","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity of MLP models — HessianMLP","text":"SensMLP object sensitivity metrics sensitivities MLP model passed function.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sensitivity of MLP models — HessianMLP","text":"case using input class factor package   need enter input data matrix, dummies must created   training neural network. , training data must given function using   trData argument.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"plots","dir":"Reference","previous_headings":"","what":"Plots","title":"Sensitivity of MLP models — HessianMLP","text":"Plot 1: colorful plot classification   classes 2D map Plot 2: b/w plot probability   chosen class 2D map Plot 3: plot stats::predictions   data provided","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessianMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity of MLP models — HessianMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 100 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #> , , 1 #>  #>            WD     TEMP #> WD   1.028311 1.174910 #> TEMP 1.174910 3.386778 #>  #> , , 2 #>  #>             WD     TEMP #> WD   -2.693044 0.856781 #> TEMP  0.856781 1.608223 #>  #> , , 3 #>  #>             WD      TEMP #> WD   -3.523861 -2.061425 #> TEMP -2.061425 -5.352261 #>  #> , , 4 #>  #>             WD      TEMP #> WD   -3.823715 -2.719650 #> TEMP -2.719650 -7.312955 #>  #> , , 5 #>  #>              WD      TEMP #> WD   -2.1583991 0.4679963 #> TEMP  0.4679963 1.9635852 #>  # \\donttest{ # Try HessianMLP to calculate sensitivities with respect to output of hidden neurones NeuralSens::HessianMLP(nnetmod, trData = nntrData,                              sens_origin_layer = 2,                              sens_end_layer = \"last\",                              sens_origin_input = FALSE,                              sens_end_input = FALSE) #> Warning: All aesthetics have length 1, but the data has 15 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #> , , 1 #>  #>             Neuron 2.1 Neuron 2.2 Neuron 2.3 Neuron 2.4 Neuron 2.5 #> Neuron 2.1 0.001619946  0.0000000 0.00000000  0.0000000 0.00000000 #> Neuron 2.2 0.000000000  0.2446066 0.00000000  0.0000000 0.00000000 #> Neuron 2.3 0.000000000  0.0000000 0.02694284  0.0000000 0.00000000 #> Neuron 2.4 0.000000000  0.0000000 0.00000000  0.0483464 0.00000000 #> Neuron 2.5 0.000000000  0.0000000 0.00000000  0.0000000 0.04601234 #>  #> , , 2 #>  #>            Neuron 2.1 Neuron 2.2 Neuron 2.3 Neuron 2.4 Neuron 2.5 #> Neuron 2.1 0.03842943 0.00000000 0.00000000  0.0000000  0.0000000 #> Neuron 2.2 0.00000000 0.09207826 0.00000000  0.0000000  0.0000000 #> Neuron 2.3 0.00000000 0.00000000 0.02007891  0.0000000  0.0000000 #> Neuron 2.4 0.00000000 0.00000000 0.00000000 -0.2799695  0.0000000 #> Neuron 2.5 0.00000000 0.00000000 0.00000000  0.0000000 -0.2605999 #>  #> , , 3 #>  #>            Neuron 2.1 Neuron 2.2 Neuron 2.3 Neuron 2.4 Neuron 2.5 #> Neuron 2.1 0.04652198  0.0000000 0.00000000  0.0000000  0.0000000 #> Neuron 2.2 0.00000000 -0.4592029 0.00000000  0.0000000  0.0000000 #> Neuron 2.3 0.00000000  0.0000000 0.01420032  0.0000000  0.0000000 #> Neuron 2.4 0.00000000  0.0000000 0.00000000 -0.2499173  0.0000000 #> Neuron 2.5 0.00000000  0.0000000 0.00000000  0.0000000 -0.2360522 #>  #> , , 4 #>  #>            Neuron 2.1 Neuron 2.2  Neuron 2.3 Neuron 2.4 Neuron 2.5 #> Neuron 2.1 0.03074436  0.0000000 0.000000000  0.0000000  0.0000000 #> Neuron 2.2 0.00000000 -0.5986865 0.000000000  0.0000000  0.0000000 #> Neuron 2.3 0.00000000  0.0000000 0.008724927  0.0000000  0.0000000 #> Neuron 2.4 0.00000000  0.0000000 0.000000000 -0.2480003  0.0000000 #> Neuron 2.5 0.00000000  0.0000000 0.000000000  0.0000000 -0.2344207 #>  #> , , 5 #>  #>            Neuron 2.1 Neuron 2.2 Neuron 2.3 Neuron 2.4 Neuron 2.5 #> Neuron 2.1 0.09249169 0.00000000 0.00000000  0.0000000  0.0000000 #> Neuron 2.2 0.00000000 0.07461993 0.00000000  0.0000000  0.0000000 #> Neuron 2.3 0.00000000 0.00000000 0.02815211  0.0000000  0.0000000 #> Neuron 2.4 0.00000000 0.00000000 0.00000000 -0.2336736  0.0000000 #> Neuron 2.5 0.00000000 0.00000000 0.00000000  0.0000000 -0.2210683 #>  ## Train caret NNET ------------------------------------------------------------ # Create trainControl ctrl_tune <- caret::trainControl(method = \"boot\",                                  savePredictions = FALSE,                                  summaryFunction = caret::defaultSummary) set.seed(150) #For replication caretmod <- caret::train(form = DEM~.,                               data = fdata.Reg.tr,                               method = \"nnet\",                               linout = TRUE,                               tuneGrid = data.frame(size = 3,                                                     decay = decay),                               maxit = iters,                               preProcess = c(\"center\",\"scale\"),                               trControl = ctrl_tune,                               metric = \"RMSE\") #> Loading required package: ggplot2 #> Loading required package: lattice #> # weights:  13 #> initial  value 448.296994  #> iter  10 value 4.240464 #> iter  20 value 2.955813 #> iter  30 value 2.432667 #> iter  40 value 2.152339 #> iter  50 value 2.119534 #> iter  60 value 2.116452 #> iter  70 value 2.115042 #> final  value 2.115024  #> converged #> # weights:  13 #> initial  value 259.256650  #> iter  10 value 5.626711 #> iter  20 value 3.294659 #> iter  30 value 2.713958 #> iter  40 value 2.218022 #> iter  50 value 2.141379 #> iter  60 value 2.100764 #> iter  70 value 2.080285 #> iter  80 value 2.075813 #> iter  90 value 2.075489 #> final  value 2.075479  #> converged #> # weights:  13 #> initial  value 839.080081  #> iter  10 value 7.558987 #> iter  20 value 5.177089 #> iter  30 value 4.017037 #> iter  40 value 2.214439 #> iter  50 value 2.059848 #> iter  60 value 2.054339 #> iter  70 value 2.045965 #> iter  80 value 2.039738 #> iter  90 value 2.015979 #> iter 100 value 1.990266 #> final  value 1.990266  #> stopped after 100 iterations #> # weights:  13 #> initial  value 2367.468092  #> iter  10 value 18.231718 #> iter  20 value 6.058120 #> iter  30 value 3.242985 #> iter  40 value 2.187840 #> iter  50 value 2.071775 #> iter  60 value 2.050664 #> iter  70 value 2.028564 #> iter  80 value 2.004616 #> iter  90 value 1.984345 #> iter 100 value 1.979182 #> final  value 1.979182  #> stopped after 100 iterations #> # weights:  13 #> initial  value 466.520607  #> iter  10 value 26.340684 #> iter  20 value 5.710540 #> iter  30 value 4.505050 #> iter  40 value 3.920448 #> iter  50 value 2.611841 #> iter  60 value 2.175444 #> iter  70 value 2.114173 #> iter  80 value 2.113462 #> iter  90 value 2.113367 #> final  value 2.113363  #> converged #> # weights:  13 #> initial  value 997.898260  #> iter  10 value 7.946815 #> iter  20 value 3.796169 #> iter  30 value 2.732974 #> iter  40 value 2.185516 #> iter  50 value 2.016426 #> iter  60 value 1.996298 #> iter  70 value 1.984364 #> iter  80 value 1.983086 #> iter  90 value 1.983051 #> final  value 1.983051  #> converged #> # weights:  13 #> initial  value 926.945659  #> iter  10 value 8.424427 #> iter  20 value 3.640577 #> iter  30 value 2.446188 #> iter  40 value 2.280445 #> iter  50 value 2.270542 #> iter  60 value 2.265967 #> iter  70 value 2.255105 #> iter  80 value 2.247681 #> iter  90 value 2.244869 #> iter 100 value 2.244534 #> final  value 2.244534  #> stopped after 100 iterations #> # weights:  13 #> initial  value 142.892088  #> iter  10 value 6.910998 #> iter  20 value 5.724961 #> iter  30 value 4.789805 #> iter  40 value 3.951995 #> iter  50 value 2.271814 #> iter  60 value 2.143636 #> iter  70 value 2.081638 #> iter  80 value 2.053068 #> iter  90 value 2.050443 #> iter 100 value 2.050366 #> final  value 2.050366  #> stopped after 100 iterations #> # weights:  13 #> initial  value 224.855827  #> iter  10 value 6.463371 #> iter  20 value 4.498606 #> iter  30 value 4.138860 #> iter  40 value 2.669166 #> iter  50 value 2.190129 #> iter  60 value 2.057145 #> iter  70 value 2.031552 #> iter  80 value 2.030720 #> iter  90 value 2.030692 #> final  value 2.030691  #> converged #> # weights:  13 #> initial  value 56.501349  #> iter  10 value 4.546989 #> iter  20 value 2.412030 #> iter  30 value 2.211975 #> iter  40 value 2.126688 #> iter  50 value 2.118982 #> iter  60 value 2.118522 #> final  value 2.118318  #> converged #> # weights:  13 #> initial  value 1537.541660  #> iter  10 value 45.893358 #> iter  20 value 14.179474 #> iter  30 value 6.293025 #> iter  40 value 2.966490 #> iter  50 value 2.250200 #> iter  60 value 2.200879 #> iter  70 value 2.132716 #> iter  80 value 2.116182 #> iter  90 value 2.113110 #> final  value 2.112921  #> converged #> # weights:  13 #> initial  value 3354.677303  #> iter  10 value 8.207666 #> iter  20 value 5.979884 #> iter  30 value 4.402456 #> iter  40 value 2.680091 #> iter  50 value 2.140337 #> iter  60 value 2.120667 #> iter  70 value 2.090521 #> iter  80 value 2.072754 #> iter  90 value 2.064807 #> iter 100 value 2.063714 #> final  value 2.063714  #> stopped after 100 iterations #> # weights:  13 #> initial  value 1495.824272  #> iter  10 value 6.518061 #> iter  20 value 4.118493 #> iter  30 value 3.189918 #> iter  40 value 2.385578 #> iter  50 value 2.157477 #> iter  60 value 2.139096 #> iter  70 value 2.136309 #> final  value 2.136306  #> converged #> # weights:  13 #> initial  value 11.757653  #> iter  10 value 3.909688 #> iter  20 value 2.556194 #> iter  30 value 2.363998 #> iter  40 value 2.217477 #> iter  50 value 2.190203 #> iter  60 value 2.185572 #> iter  70 value 2.183208 #> final  value 2.183205  #> converged #> # weights:  13 #> initial  value 154.922979  #> iter  10 value 5.007005 #> iter  20 value 2.986467 #> iter  30 value 2.324922 #> iter  40 value 2.096300 #> iter  50 value 2.077648 #> iter  60 value 2.071380 #> iter  70 value 2.070058 #> final  value 2.070057  #> converged #> # weights:  13 #> initial  value 1295.102618  #> iter  10 value 20.587942 #> iter  20 value 5.648115 #> iter  30 value 4.498304 #> iter  40 value 2.651133 #> iter  50 value 2.284889 #> iter  60 value 2.195444 #> iter  70 value 2.167062 #> iter  80 value 2.151623 #> iter  90 value 2.147625 #> final  value 2.147598  #> converged #> # weights:  13 #> initial  value 6196.422374  #> iter  10 value 9.791647 #> iter  20 value 8.194195 #> iter  30 value 5.683012 #> iter  40 value 2.600464 #> iter  50 value 2.259054 #> iter  60 value 2.197961 #> iter  70 value 2.156120 #> iter  80 value 2.116732 #> iter  90 value 2.099782 #> iter 100 value 2.096804 #> final  value 2.096804  #> stopped after 100 iterations #> # weights:  13 #> initial  value 165.586684  #> iter  10 value 6.965082 #> iter  20 value 2.942559 #> iter  30 value 2.400000 #> iter  40 value 2.192173 #> iter  50 value 2.147873 #> iter  60 value 2.145626 #> iter  70 value 2.144777 #> final  value 2.144775  #> converged #> # weights:  13 #> initial  value 3685.490536  #> iter  10 value 7.497643 #> iter  20 value 4.678847 #> iter  30 value 3.198315 #> iter  40 value 2.332407 #> iter  50 value 2.266814 #> iter  60 value 2.246946 #> iter  70 value 2.230340 #> iter  80 value 2.225874 #> iter  90 value 2.223878 #> final  value 2.223678  #> converged #> # weights:  13 #> initial  value 1883.675021  #> iter  10 value 9.414955 #> iter  20 value 6.244387 #> iter  30 value 3.029821 #> iter  40 value 2.441478 #> iter  50 value 2.388556 #> iter  60 value 2.310839 #> iter  70 value 2.223355 #> iter  80 value 2.200783 #> iter  90 value 2.161414 #> iter 100 value 2.149199 #> final  value 2.149199  #> stopped after 100 iterations #> # weights:  13 #> initial  value 2896.671370  #> iter  10 value 5.948826 #> iter  20 value 3.887106 #> iter  30 value 2.866152 #> iter  40 value 2.358066 #> iter  50 value 2.269247 #> iter  60 value 2.244193 #> iter  70 value 2.236110 #> iter  80 value 2.235692 #> final  value 2.235688  #> converged #> # weights:  13 #> initial  value 1160.170910  #> iter  10 value 31.153142 #> iter  20 value 11.439016 #> iter  30 value 5.790787 #> iter  40 value 3.100087 #> iter  50 value 2.444872 #> iter  60 value 2.285386 #> iter  70 value 2.154482 #> iter  80 value 2.141414 #> iter  90 value 2.136232 #> final  value 2.135919  #> converged #> # weights:  13 #> initial  value 3189.626236  #> iter  10 value 5.430796 #> iter  20 value 3.625953 #> iter  30 value 2.730844 #> iter  40 value 2.340266 #> iter  50 value 2.247853 #> iter  60 value 2.174153 #> iter  70 value 2.141220 #> iter  80 value 2.127328 #> iter  90 value 2.098656 #> iter 100 value 2.071428 #> final  value 2.071428  #> stopped after 100 iterations #> # weights:  13 #> initial  value 316.924838  #> iter  10 value 5.937193 #> iter  20 value 4.755514 #> iter  30 value 3.663735 #> iter  40 value 2.233143 #> iter  50 value 2.134806 #> iter  60 value 2.113021 #> iter  70 value 2.101975 #> iter  80 value 2.100327 #> iter  90 value 2.100020 #> final  value 2.100013  #> converged #> # weights:  13 #> initial  value 1475.824755  #> iter  10 value 6.897328 #> iter  20 value 5.017823 #> iter  30 value 3.550261 #> iter  40 value 2.277808 #> iter  50 value 2.126565 #> iter  60 value 2.103660 #> iter  70 value 2.089784 #> iter  80 value 2.089397 #> final  value 2.089397  #> converged #> # weights:  13 #> initial  value 2705.392001  #> iter  10 value 25.983461 #> iter  20 value 9.945895 #> iter  30 value 4.266584 #> iter  40 value 3.251963 #> iter  50 value 2.661341 #> iter  60 value 2.218021 #> iter  70 value 2.117866 #> iter  80 value 2.104051 #> iter  90 value 2.101507 #> final  value 2.101324  #> converged  # Try HessianMLP NeuralSens::HessianMLP(caretmod) #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-3-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #> , , 1 #>  #>                WD         TEMP #> WD   0.0080548740 0.0006507174 #> TEMP 0.0006507174 0.0501312386 #>  #> , , 2 #>  #>                WD        TEMP #> WD   -0.003715874 0.002293548 #> TEMP  0.002293548 0.058358834 #>  #> , , 3 #>  #>                WD        TEMP #> WD   -0.005441919 0.002616204 #> TEMP  0.002616204 0.049183293 #>  #> , , 4 #>  #>                WD        TEMP #> WD   -0.005505955 0.002723758 #> TEMP  0.002723758 0.032930386 #>  #> , , 5 #>  #>                WD        TEMP #> WD   -0.005709236 0.002436998 #> TEMP  0.002436998 0.073151732 #>   ## Train h2o NNET -------------------------------------------------------------- # Create a cluster with 4 available cores h2o::h2o.init(ip = \"localhost\",               nthreads = 4) #>  #> H2O is not running yet, starting it now... #>  #> Note:  In case of errors look at the following log files: #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file16507ec44e0e/h2o_jpizarroso_started_from_r.out #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file16504fd83d73/h2o_jpizarroso_started_from_r.err #>  #>  #> Starting H2O JVM and connecting:  Connection successful! #>  #> R is connected to the H2O cluster:  #>     H2O cluster uptime:         3 seconds 87 milliseconds  #>     H2O cluster timezone:       Europe/Madrid  #>     H2O data parsing timezone:  UTC  #>     H2O cluster version:        3.44.0.3  #>     H2O cluster version age:    6 months and 4 days  #>     H2O cluster name:           H2O_started_from_R_jpizarroso_inw091  #>     H2O cluster total nodes:    1  #>     H2O cluster total memory:   7.91 GB  #>     H2O cluster total cores:    20  #>     H2O cluster allowed cores:  4  #>     H2O cluster healthy:        TRUE  #>     H2O Connection ip:          localhost  #>     H2O Connection port:        54321  #>     H2O Connection proxy:       NA  #>     H2O Internal Security:      FALSE  #>     R Version:                  R version 4.3.3 (2024-02-29 ucrt)  #> Warning:  #> Your H2O cluster version is (6 months and 4 days) old. There may be a newer version available. #> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html #>   # Reset the cluster h2o::h2o.removeAll() fdata_h2o <- h2o::as.h2o(x = fdata.Reg.tr, destination_frame = \"fdata_h2o\") #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  set.seed(150) h2omod <-h2o:: h2o.deeplearning(x = names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)],                                      y = names(fdata.Reg.tr)[1],                                      distribution = \"AUTO\",                                      training_frame = fdata_h2o,                                      standardize = TRUE,                                      activation = \"Tanh\",                                      hidden = c(hidden_neurons),                                      stopping_rounds = 0,                                      epochs = iters,                                      seed = 150,                                      model_id = \"nnet_h2o\",                                      adaptive_rate = FALSE,                                      rate_decay = decay,                                      export_weights_and_biases = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Try HessianMLP NeuralSens::HessianMLP(h2omod) #> Error in value[[3L]](cond): The training data has not been detected, load the data to the h2o cluster  # Turn off the cluster h2o::h2o.shutdown(prompt = FALSE) rm(fdata_h2o)  ## Train RSNNS NNET ------------------------------------------------------------ # Normalize data using RSNNS algorithms trData <- as.data.frame(RSNNS::normalizeData(fdata.Reg.tr)) names(trData) <- names(fdata.Reg.tr) set.seed(150) RSNNSmod <-RSNNS::mlp(x = trData[,2:ncol(trData)],                            y = trData[,1],                            size = hidden_neurons,                            linOut = TRUE,                            learnFuncParams=c(decay),                            maxit=iters)  # Try HessianMLP NeuralSens::HessianMLP(RSNNSmod, trData = trData, output_name = \"DEM\") #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`?  #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $DEM #> , , 1 #>  #>               WD       TEMP #> WD   0.002575086 0.03535806 #> TEMP 0.035358060 1.15458447 #>  #> , , 2 #>  #>               WD       TEMP #> WD   -0.61096374 0.06024617 #> TEMP  0.06024617 0.31091788 #>  #> , , 3 #>  #>               WD        TEMP #> WD   -0.53952940 -0.02698437 #> TEMP -0.02698437 -0.78909969 #>  #> , , 4 #>  #>               WD        TEMP #> WD   -0.53538154 -0.09282737 #> TEMP -0.09282737 -1.70730704 #>  #> , , 5 #>  #>              WD      TEMP #> WD   -0.5105605 0.1222717 #> TEMP  0.1222717 1.3438703 #>   ## USE DEFAULT METHOD ---------------------------------------------------------- NeuralSens::HessianMLP(caretmod$finalModel$wts,                             trData = fdata.Reg.tr,                             mlpstr = caretmod$finalModel$n,                             coefnames = caretmod$coefnames,                             actfun = c(\"linear\",\"sigmoid\",\"linear\"),                             output_name = \"DEM\") #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-3-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $DEM #> , , 1 #>  #>               WD        TEMP #> WD   -0.00464978  0.00231262 #> TEMP  0.00231262 -0.03172112 #>  #> , , 2 #>  #>                WD         TEMP #> WD   -0.005545458  0.002480041 #> TEMP  0.002480041 -0.031254782 #>  #> , , 3 #>  #>                WD         TEMP #> WD   -0.005627625  0.002447588 #> TEMP  0.002447588 -0.032088962 #>  #> , , 4 #>  #>                WD         TEMP #> WD   -0.005558544  0.002355347 #> TEMP  0.002355347 -0.033006047 #>  #> , , 5 #>  #>                WD         TEMP #> WD   -0.005754328  0.002578274 #> TEMP  0.002578274 -0.029533354 #>   ################################################################################ #########################  CLASSIFICATION NNET ################################# ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.cl <- fdata[,2:ncol(fdata)] fdata.Reg.cl[,2:3] <- fdata.Reg.cl[,2:3]/10 fdata.Reg.cl[,1] <- fdata.Reg.cl[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.cl, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.cl)  # Factorize the output fdata.Reg.cl$DEM <- factor(round(fdata.Reg.cl$DEM, digits = 1))  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.cl, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.cl)  ## Train caret NNET ------------------------------------------------------------ # Create trainControl ctrl_tune <- caret::trainControl(method = \"boot\",                                  savePredictions = FALSE,                                  summaryFunction = caret::defaultSummary) set.seed(150) #For replication caretmod <- caret::train(form = DEM~.,                                 data = fdata.Reg.cl,                                 method = \"nnet\",                                 linout = FALSE,                                 tuneGrid = data.frame(size = hidden_neurons,                                                       decay = decay),                                 maxit = iters,                                 preProcess = c(\"center\",\"scale\"),                                 trControl = ctrl_tune,                                 metric = \"Accuracy\") #> # weights:  45 #> initial  value 3772.842618  #> iter  10 value 1557.564302 #> iter  20 value 1214.199134 #> iter  30 value 1110.296648 #> iter  40 value 1070.982383 #> iter  50 value 1040.282432 #> iter  60 value 1026.243143 #> iter  70 value 1022.185164 #> iter  80 value 1018.932608 #> iter  90 value 1014.806386 #> iter 100 value 1012.499658 #> final  value 1012.499658  #> stopped after 100 iterations #> # weights:  45 #> initial  value 2668.172951  #> iter  10 value 1496.742417 #> iter  20 value 1187.526898 #> iter  30 value 1050.272689 #> iter  40 value 1013.322223 #> iter  50 value 996.639753 #> iter  60 value 979.316887 #> iter  70 value 969.798307 #> iter  80 value 966.613930 #> iter  90 value 965.693488 #> iter 100 value 965.463123 #> final  value 965.463123  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3297.203342  #> iter  10 value 1377.287449 #> iter  20 value 1112.310755 #> iter  30 value 1023.822455 #> iter  40 value 991.843628 #> iter  50 value 956.727170 #> iter  60 value 941.910334 #> iter  70 value 936.609226 #> iter  80 value 935.410703 #> iter  90 value 935.163698 #> iter 100 value 935.127337 #> final  value 935.127337  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3568.399296  #> iter  10 value 1489.763894 #> iter  20 value 1219.257218 #> iter  30 value 1076.771253 #> iter  40 value 1024.503423 #> iter  50 value 998.708032 #> iter  60 value 981.840999 #> iter  70 value 973.720379 #> iter  80 value 971.090791 #> iter  90 value 968.967523 #> iter 100 value 968.494390 #> final  value 968.494390  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3003.535668  #> iter  10 value 1439.044442 #> iter  20 value 1176.600696 #> iter  30 value 1048.481267 #> iter  40 value 1016.572547 #> iter  50 value 1011.297145 #> iter  60 value 999.404464 #> iter  70 value 988.451143 #> iter  80 value 977.282991 #> iter  90 value 972.906275 #> iter 100 value 971.896597 #> final  value 971.896597  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3408.995747  #> iter  10 value 1341.622904 #> iter  20 value 1089.969900 #> iter  30 value 1029.207268 #> iter  40 value 996.486089 #> iter  50 value 988.952619 #> iter  60 value 976.464472 #> iter  70 value 972.214434 #> iter  80 value 970.881761 #> iter  90 value 969.641644 #> iter 100 value 969.485114 #> final  value 969.485114  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4281.102149  #> iter  10 value 1513.066785 #> iter  20 value 1329.056483 #> iter  30 value 1209.982389 #> iter  40 value 1118.764207 #> iter  50 value 1076.036468 #> iter  60 value 1061.718419 #> iter  70 value 1050.693937 #> iter  80 value 1040.872146 #> iter  90 value 1029.507488 #> iter 100 value 1025.374541 #> final  value 1025.374541  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3265.828022  #> iter  10 value 1456.434584 #> iter  20 value 1166.808947 #> iter  30 value 1057.977863 #> iter  40 value 1008.996008 #> iter  50 value 978.114751 #> iter  60 value 970.985521 #> iter  70 value 966.104095 #> iter  80 value 963.006874 #> iter  90 value 962.094607 #> iter 100 value 961.841612 #> final  value 961.841612  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3019.717560  #> iter  10 value 1333.523881 #> iter  20 value 1154.064664 #> iter  30 value 1067.529755 #> iter  40 value 1054.332888 #> iter  50 value 1049.493394 #> iter  60 value 1042.420751 #> iter  70 value 1025.594053 #> iter  80 value 1018.729658 #> iter  90 value 1016.946541 #> iter 100 value 1015.397734 #> final  value 1015.397734  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3405.760894  #> iter  10 value 1597.431168 #> iter  20 value 1263.793552 #> iter  30 value 1114.159495 #> iter  40 value 1048.729401 #> iter  50 value 1023.871076 #> iter  60 value 976.998748 #> iter  70 value 960.770263 #> iter  80 value 954.884250 #> iter  90 value 952.488444 #> iter 100 value 951.344477 #> final  value 951.344477  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3013.688100  #> iter  10 value 1535.862594 #> iter  20 value 1231.453772 #> iter  30 value 1132.096002 #> iter  40 value 1106.684078 #> iter  50 value 1083.676956 #> iter  60 value 1041.694085 #> iter  70 value 1032.125938 #> iter  80 value 1022.027396 #> iter  90 value 1006.266589 #> iter 100 value 1000.802958 #> final  value 1000.802958  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4628.455991  #> iter  10 value 1552.025703 #> iter  20 value 1211.906415 #> iter  30 value 1084.177292 #> iter  40 value 1064.063606 #> iter  50 value 1055.265752 #> iter  60 value 1049.279896 #> iter  70 value 1040.272607 #> iter  80 value 1038.639372 #> iter  90 value 1038.099537 #> iter 100 value 1037.787531 #> final  value 1037.787531  #> stopped after 100 iterations #> # weights:  45 #> initial  value 2860.694342  #> iter  10 value 1511.390857 #> iter  20 value 1168.419352 #> iter  30 value 1094.111978 #> iter  40 value 1073.168803 #> iter  50 value 1069.749263 #> iter  60 value 1066.180221 #> iter  70 value 1064.629158 #> iter  80 value 1064.226117 #> iter  90 value 1063.853095 #> iter 100 value 1062.896671 #> final  value 1062.896671  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3377.433754  #> iter  10 value 1446.109214 #> iter  20 value 1171.013957 #> iter  30 value 1114.422290 #> iter  40 value 1085.906251 #> iter  50 value 1056.654790 #> iter  60 value 1043.204252 #> iter  70 value 1039.982529 #> iter  80 value 1037.244251 #> iter  90 value 1036.464530 #> iter 100 value 1036.224249 #> final  value 1036.224249  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3253.715832  #> iter  10 value 1411.560701 #> iter  20 value 1173.605210 #> iter  30 value 1141.598323 #> iter  40 value 1110.027489 #> iter  50 value 1075.678228 #> iter  60 value 1065.287014 #> iter  70 value 1061.607689 #> iter  80 value 1054.627824 #> iter  90 value 1034.673250 #> iter 100 value 1029.300197 #> final  value 1029.300197  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3863.665785  #> iter  10 value 1387.778891 #> iter  20 value 1141.991241 #> iter  30 value 1113.544623 #> iter  40 value 1093.112775 #> iter  50 value 1064.466827 #> iter  60 value 1053.501083 #> iter  70 value 1035.802924 #> iter  80 value 1020.443915 #> iter  90 value 1015.693675 #> iter 100 value 1014.111725 #> final  value 1014.111725  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3378.665527  #> iter  10 value 1400.194414 #> iter  20 value 1134.238327 #> iter  30 value 1072.434905 #> iter  40 value 1022.352730 #> iter  50 value 1009.326061 #> iter  60 value 1003.254602 #> iter  70 value 998.622585 #> iter  80 value 996.924622 #> iter  90 value 995.431517 #> iter 100 value 995.296204 #> final  value 995.296204  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3778.883814  #> iter  10 value 1287.189554 #> iter  20 value 1085.911856 #> iter  30 value 1048.258887 #> iter  40 value 1031.285258 #> iter  50 value 1020.746772 #> iter  60 value 1009.982512 #> iter  70 value 1001.117193 #> iter  80 value 996.249854 #> iter  90 value 995.002293 #> iter 100 value 994.464160 #> final  value 994.464160  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3120.123677  #> iter  10 value 1354.765028 #> iter  20 value 1127.326225 #> iter  30 value 1030.192879 #> iter  40 value 995.812629 #> iter  50 value 987.332787 #> iter  60 value 980.506578 #> iter  70 value 976.439833 #> iter  80 value 975.171226 #> iter  90 value 974.762849 #> iter 100 value 974.669379 #> final  value 974.669379  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4498.743913  #> iter  10 value 1444.335556 #> iter  20 value 1196.519410 #> iter  30 value 1099.018244 #> iter  40 value 1029.077344 #> iter  50 value 1005.258084 #> iter  60 value 997.634916 #> iter  70 value 996.394696 #> iter  80 value 995.712015 #> iter  90 value 995.645881 #> iter 100 value 995.634593 #> final  value 995.634593  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4178.969476  #> iter  10 value 1484.773196 #> iter  20 value 1236.558325 #> iter  30 value 1131.376400 #> iter  40 value 1058.462683 #> iter  50 value 1018.919757 #> iter  60 value 1006.929988 #> iter  70 value 1000.924164 #> iter  80 value 997.057751 #> iter  90 value 992.146696 #> iter 100 value 990.467451 #> final  value 990.467451  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3763.027227  #> iter  10 value 1502.580970 #> iter  20 value 1286.900665 #> iter  30 value 1169.576206 #> iter  40 value 1122.939531 #> iter  50 value 1108.972049 #> iter  60 value 1104.953532 #> iter  70 value 1103.270073 #> iter  80 value 1101.645238 #> iter  90 value 1098.946203 #> iter 100 value 1095.719635 #> final  value 1095.719635  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3634.283962  #> iter  10 value 1393.174055 #> iter  20 value 1064.065558 #> iter  30 value 998.290332 #> iter  40 value 973.809987 #> iter  50 value 963.302264 #> iter  60 value 949.245382 #> iter  70 value 943.582074 #> iter  80 value 942.712725 #> iter  90 value 942.090755 #> iter 100 value 941.998670 #> final  value 941.998670  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4122.006202  #> iter  10 value 1440.624472 #> iter  20 value 1201.124592 #> iter  30 value 1081.368683 #> iter  40 value 1019.710036 #> iter  50 value 998.220181 #> iter  60 value 991.313592 #> iter  70 value 988.455638 #> iter  80 value 986.674128 #> iter  90 value 985.122136 #> iter 100 value 984.455611 #> final  value 984.455611  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3169.917474  #> iter  10 value 1708.163015 #> iter  20 value 1275.618128 #> iter  30 value 1086.379758 #> iter  40 value 1017.730370 #> iter  50 value 994.236230 #> iter  60 value 984.400401 #> iter  70 value 982.537214 #> iter  80 value 981.070886 #> iter  90 value 980.488255 #> iter 100 value 980.416752 #> final  value 980.416752  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3805.876854  #> iter  10 value 1440.491291 #> iter  20 value 1164.498683 #> iter  30 value 1068.693224 #> iter  40 value 1030.794446 #> iter  50 value 1012.456002 #> iter  60 value 1005.801464 #> iter  70 value 1003.516310 #> iter  80 value 1002.997394 #> iter  90 value 1002.812312 #> iter 100 value 1002.764592 #> final  value 1002.764592  #> stopped after 100 iterations  # Try HessianMLP NeuralSens::HessianMLP(caretmod) #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-5-5 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $X0.6 #> , , 1 #>  #>             WD     TEMP #> WD   -4.670997 5.172555 #> TEMP -4.879342 5.640700 #>  #> , , 2 #>  #>             WD      TEMP #> WD   -1.989823 -3.100539 #> TEMP  4.604024  5.368306 #>  #> , , 3 #>  #>             WD      TEMP #> WD   -2.164994 -2.821268 #> TEMP  4.745238  4.965177 #>  #> , , 4 #>  #>             WD      TEMP #> WD   -2.059171 -2.542789 #> TEMP  4.353979  4.768402 #>  #> , , 5 #>  #>              WD        TEMP #> WD   -0.6950128  0.08701548 #> TEMP  2.0283312 -0.15713481 #>  #> $X0.7 #> , , 1 #>  #>             WD     TEMP #> WD   -4.879342 2.125629 #> TEMP -5.349701 2.370279 #>  #> , , 2 #>  #>             WD      TEMP #> WD    4.604024 -1.335003 #> TEMP -6.963008 -2.934393 #>  #> , , 3 #>  #>             WD      TEMP #> WD    4.745238 -2.758534 #> TEMP -7.137206 -5.365716 #>  #> , , 4 #>  #>             WD      TEMP #> WD    4.353979 -2.435074 #> TEMP -6.712240 -4.405385 #>  #> , , 5 #>  #>             WD      TEMP #> WD    2.028331 -1.676425 #> TEMP -2.032015 -5.638907 #>  #> $X0.8 #> , , 1 #>  #>            WD     TEMP #> WD   1.800258 2.370279 #> TEMP 1.792172 2.685159 #>  #> , , 2 #>  #>             WD      TEMP #> WD   0.8402014 -2.934393 #> TEMP 3.4258523 -2.048769 #>  #> , , 3 #>  #>            WD      TEMP #> WD   2.517725 -5.365716 #> TEMP 6.356315 -4.476028 #>  #> , , 4 #>  #>            WD      TEMP #> WD   2.163635 -4.405385 #> TEMP 5.357798 -4.079587 #>  #> , , 5 #>  #>            WD      TEMP #> WD   1.498457 -5.638907 #> TEMP 6.115961 -2.092491 #>  #> $X0.5 #> , , 1 #>  #>            WD      TEMP #> WD   1.792172 -4.159239 #> TEMP 1.697777 -4.457715 #>  #> , , 2 #>  #>            WD      TEMP #> WD   3.425852  3.013873 #> TEMP 1.632748 -1.992721 #>  #> , , 3 #>  #>            WD      TEMP #> WD   6.356315  3.729933 #> TEMP 4.623315 -2.913048 #>  #> , , 4 #>  #>            WD      TEMP #> WD   5.357798  3.538264 #> TEMP 4.108822 -2.762054 #>  #> , , 5 #>  #>            WD      TEMP #> WD   6.115961  3.874222 #> TEMP 2.727358 -2.592702 #>  #> $X0.9 #> , , 1 #>  #>            WD      TEMP #> WD   4.902325 -4.457715 #> TEMP 5.172555 -4.676172 #>  #> , , 2 #>  #>              WD      TEMP #> WD   -0.5312368 -1.992721 #> TEMP -3.1005392  2.004980 #>  #> , , 3 #>  #>             WD      TEMP #> WD   -1.326780 -2.913048 #> TEMP -2.821268  2.017951 #>  #> , , 4 #>  #>             WD      TEMP #> WD   -1.210080 -2.762054 #> TEMP -2.542789  1.908331 #>  #> , , 5 #>  #>               WD      TEMP #> WD   -3.00259510 -2.592702 #> TEMP  0.08701548  1.551614 #>   ## Train h2o NNET -------------------------------------------------------------- # Create local cluster with 4 available cores h2o::h2o.init(ip = \"localhost\",               nthreads = 4) #>  #> H2O is not running yet, starting it now... #>  #> Note:  In case of errors look at the following log files: #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file165041d143da/h2o_jpizarroso_started_from_r.out #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file1650384212f2/h2o_jpizarroso_started_from_r.err #>  #>  #> Starting H2O JVM and connecting:  Connection successful! #>  #> R is connected to the H2O cluster:  #>     H2O cluster uptime:         2 seconds 741 milliseconds  #>     H2O cluster timezone:       Europe/Madrid  #>     H2O data parsing timezone:  UTC  #>     H2O cluster version:        3.44.0.3  #>     H2O cluster version age:    6 months and 4 days  #>     H2O cluster name:           H2O_started_from_R_jpizarroso_vno719  #>     H2O cluster total nodes:    1  #>     H2O cluster total memory:   7.91 GB  #>     H2O cluster total cores:    20  #>     H2O cluster allowed cores:  4  #>     H2O cluster healthy:        TRUE  #>     H2O Connection ip:          localhost  #>     H2O Connection port:        54321  #>     H2O Connection proxy:       NA  #>     H2O Internal Security:      FALSE  #>     R Version:                  R version 4.3.3 (2024-02-29 ucrt)  #> Warning:  #> Your H2O cluster version is (6 months and 4 days) old. There may be a newer version available. #> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html #>   # Reset the cluster h2o::h2o.removeAll() fdata_h2o <- h2o::as.h2o(x = fdata.Reg.cl, destination_frame = \"fdata_h2o\") #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  set.seed(150) h2omod <- h2o::h2o.deeplearning(x = names(fdata.Reg.cl)[2:ncol(fdata.Reg.cl)],                                        y = names(fdata.Reg.cl)[1],                                        distribution = \"AUTO\",                                        training_frame = fdata_h2o,                                        standardize = TRUE,                                        activation = \"Tanh\",                                        hidden = c(hidden_neurons),                                        stopping_rounds = 0,                                        epochs = iters,                                        seed = 150,                                        model_id = \"nnet_h2o\",                                        adaptive_rate = FALSE,                                        rate_decay = decay,                                        export_weights_and_biases = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Try HessianMLP NeuralSens::HessianMLP(h2omod) #> Error in value[[3L]](cond): The training data has not been detected, load the data to the h2o cluster  # Apaga el cluster h2o::h2o.shutdown(prompt = FALSE) rm(fdata_h2o)  ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  45 #> initial  value 3571.076907  #> iter  10 value 1391.280224 #> iter  20 value 1195.078969 #> iter  30 value 1148.503871 #> iter  40 value 1071.086906 #> iter  50 value 1016.236754 #> iter  60 value 1003.803225 #> iter  70 value 998.327935 #> iter  80 value 996.114736 #> iter  90 value 995.001087 #> iter 100 value 994.799975 #> final  value 994.799975  #> stopped after 100 iterations # Try HessianMLP NeuralSens::HessianMLP(nnetmod, trData = nntrData) #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-5-5 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $X0.6 #> , , 1 #>  #>             WD     TEMP #> WD   -4.704517 4.545073 #> TEMP -5.204573 8.204524 #>  #> , , 2 #>  #>            WD      TEMP #> WD   3.636482 -1.520392 #> TEMP 1.402979 -1.715242 #>  #> , , 3 #>  #>             WD      TEMP #> WD   1.9366770 0.7298632 #> TEMP 0.4339409 2.0236421 #>  #> , , 4 #>  #>               WD     TEMP #> WD    1.84696744 0.893106 #> TEMP -0.05131954 1.886014 #>  #> , , 5 #>  #>            WD       TEMP #> WD   2.039427 -0.8577403 #> TEMP 1.823289  0.6838412 #>  #> $X0.7 #> , , 1 #>  #>              WD      TEMP #> WD    -5.204573 2.0904945 #> TEMP -10.231372 0.8312931 #>  #> , , 2 #>  #>            WD       TEMP #> WD   1.402979 -2.8512509 #> TEMP 2.042167 -0.4214128 #>  #> , , 3 #>  #>              WD      TEMP #> WD    0.4339409 -5.388502 #> TEMP -0.3122699 -5.900168 #>  #> , , 4 #>  #>               WD      TEMP #> WD   -0.05131954 -4.923406 #> TEMP -0.28683943 -4.889934 #>  #> , , 5 #>  #>              WD      TEMP #> WD    1.8232888 -3.272029 #> TEMP -0.4317642 -3.748964 #>  #> $X0.8 #> , , 1 #>  #>               WD      TEMP #> WD   -0.93161275 0.8312931 #> TEMP -0.09548594 2.3962172 #>  #> , , 2 #>  #>             WD       TEMP #> WD   1.5124632 -0.4214128 #> TEMP 0.2533784 -0.1304020 #>  #> , , 3 #>  #>            WD      TEMP #> WD   3.794342 -5.900168 #> TEMP 4.618171 -7.902479 #>  #> , , 4 #>  #>            WD      TEMP #> WD   3.438228 -4.889934 #> TEMP 3.886478 -7.157341 #>  #> , , 5 #>  #>            WD      TEMP #> WD   2.151412 -3.748964 #> TEMP 2.769673 -2.309080 #>  #> $X0.5 #> , , 1 #>  #>               WD       TEMP #> WD   -0.09548594  0.6732030 #> TEMP -0.66859269 -0.2025328 #>  #> , , 2 #>  #>              WD       TEMP #> WD    0.2533784 -1.2382507 #> TEMP -0.1375629  0.3305278 #>  #> , , 3 #>  #>            WD        TEMP #> WD   4.618171 -0.84941369 #> TEMP 6.214149  0.09189471 #>  #> , , 4 #>  #>            WD       TEMP #> WD   3.886478 -0.8679072 #> TEMP 5.620583  0.1327155 #>  #> , , 5 #>  #>            WD        TEMP #> WD   2.769673 -0.66905624 #> TEMP 1.876398  0.03313204 #>  #> $X0.9 #> , , 1 #>  #>            WD        TEMP #> WD   2.806829 -0.20253282 #> TEMP 4.545073  0.07869205 #>  #> , , 2 #>  #>             WD         TEMP #> WD   -1.050304  0.330527772 #> TEMP -1.520392 -0.009908438 #>  #> , , 3 #>  #>             WD        TEMP #> WD   0.4758575  0.09189471 #> TEMP 0.7298632 -0.08771664 #>  #> , , 4 #>  #>             WD       TEMP #> WD   0.4755354  0.1327155 #> TEMP 0.8931060 -0.1235752 #>  #> , , 5 #>  #>              WD       TEMP #> WD   -0.2561691 0.03313204 #> TEMP -0.8577403 0.16450131 #>  # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Constructor of the HessMLP Class — HessMLP","title":"Constructor of the HessMLP Class — HessMLP","text":"Create object HessMLP class","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constructor of the HessMLP Class — HessMLP","text":"","code":"HessMLP(   sens = list(),   raw_sens = list(),   mlp_struct = numeric(),   trData = data.frame(),   coefnames = character(),   output_name = character() )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constructor of the HessMLP Class — HessMLP","text":"sens list sensitivity measures, one list per output neuron raw_sens list sensitivities, one array per output neuron mlp_struct numeric vector describing structur MLP model trData data.frame data used calculate sensitivities coefnames character vector name predictor(s) output_name character vector name output(s)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constructor of the HessMLP Class — HessMLP","text":"HessMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessToSensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a HessMLP to a SensMLP object — HessToSensMLP","title":"Convert a HessMLP to a SensMLP object — HessToSensMLP","text":"Auxiliary function turn HessMLP object SensMLP object order use plot-related functions associated SensMLP","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessToSensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a HessMLP to a SensMLP object — HessToSensMLP","text":"","code":"HessToSensMLP(x)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessToSensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a HessMLP to a SensMLP object — HessToSensMLP","text":"x HessMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/HessToSensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a HessMLP to a SensMLP object — HessToSensMLP","text":"SensMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if object is of class HessMLP — is.HessMLP","title":"Check if object is of class HessMLP — is.HessMLP","text":"Check object class HessMLP","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if object is of class HessMLP — is.HessMLP","text":"","code":"is.HessMLP(object)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if object is of class HessMLP — is.HessMLP","text":"object HessMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.HessMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if object is of class HessMLP — is.HessMLP","text":"TRUE object HessMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if object is of class SensMLP — is.SensMLP","title":"Check if object is of class SensMLP — is.SensMLP","text":"Check object class SensMLP","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if object is of class SensMLP — is.SensMLP","text":"","code":"is.SensMLP(object)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if object is of class SensMLP — is.SensMLP","text":"object SensMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.SensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if object is of class SensMLP — is.SensMLP","text":"TRUE object SensMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/is.SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Check if object is of class SensMLP — is.SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"function implements k-stepM algorithm multiple hypothesis testing. tests hypothesis using critical value calculated ECDF k-max differences, updating critical value, iterating hypotheses tested.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"","code":"kStepMAlgorithm(original_stats, bootstrap_stats, num_hypotheses, alpha, k)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"original_stats numeric vector original test statistics hypothesis. bootstrap_stats numeric matrix bootstrap test statistics, rows representing bootstrap samples columns representing hypotheses. num_hypotheses integer specifying total number hypotheses. alpha numeric value specifying significance level. k integer specifying threshold number controlling k-familywise error rate.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"list containing two elements: 'signif', logical vector indicating hypotheses         rejected, 'cv', numeric vector critical values used hypothesis.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"Romano, Joseph P., Azeem M. Shaikh, Michael Wolf. \"Formalized data snooping based generalized error rates.\" Econometric Theory 24.2 (2008): 404-447.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/kStepMAlgorithm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-StepM Algorithm for Hypothesis Testing — kStepMAlgorithm","text":"","code":"original_stats <- rnorm(10) bootstrap_stats <- matrix(rnorm(1000), ncol = 10) result <- kStepMAlgorithm(original_stats, bootstrap_stats, 10, 0.05, 1)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/NeuralSens.html","id":null,"dir":"Reference","previous_headings":"","what":"NeuralSens: Sensitivity Analysis of Neural Networks — NeuralSens","title":"NeuralSens: Sensitivity Analysis of Neural Networks — NeuralSens","text":"Visualization analysis tools aid interpretation neural network models.","code":""},{"path":[]},{"path":"https://jaipizgon.github.io/NeuralSens/reference/NeuralSens.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"NeuralSens: Sensitivity Analysis of Neural Networks — NeuralSens","text":"Maintainer: Jaime Pizarroso Gonzalo jpizarroso@comillas.edu [contributor] Authors: José Portela González Jose.Portela@iit.comillas.edu Antonio Muñoz San Roque antonio.munoz@iit.comillas.edu","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for the HessMLP Class — plot.HessMLP","title":"Plot method for the HessMLP Class — plot.HessMLP","text":"Plot sensitivities sensitivity metrics HessMLP object.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for the HessMLP Class — plot.HessMLP","text":"","code":"# S3 method for HessMLP plot(   x,   plotType = c(\"sensitivities\", \"time\", \"features\", \"matrix\", \"interactions\"),   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for the HessMLP Class — plot.HessMLP","text":"x HessMLP object created HessianMLP plotType character specifying type plot created. can : \"sensitivities\" (default): use HessianMLP function \"time\": use SensTimePlot function \"features\": use  HessFeaturePlot function \"matrix\": use SensMatPlot function show values      second partial derivatives \"interactions\": use SensMatPlot function show      values second partial derivatives first partial derivatives diagonal ... additional parameters passed plot function NeuralSens package","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.HessMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot method for the HessMLP Class — plot.HessMLP","text":"list graphic objects created ggplot","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.HessMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot method for the HessMLP Class — plot.HessMLP","text":"","code":"#' ## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP sens <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) # \\donttest{ plot(sens) #> Warning: All aesthetics have length 1, but the data has 3 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  plot(sens,\"time\") #> Warning: Use of `plotdata$value` is discouraged. #> ℹ Use `value` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead.  # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for the SensMLP Class — plot.SensMLP","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"Plot sensitivities sensitivity metrics SensMLP object.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"","code":"# S3 method for SensMLP plot(x, plotType = c(\"sensitivities\", \"time\", \"features\"), ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"x SensMLP object created SensAnalysisMLP plotType character specifying type plot created. can : \"sensitivities\" (default): use SensAnalysisMLP function \"time\": use SensTimePlot function \"features\": use  SensFeaturePlot function ... additional parameters passed plot function NeuralSens package","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"list graphic objects created ggplot","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/plot.SensMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot method for the SensMLP Class — plot.SensMLP","text":"","code":"#' ## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) # \\donttest{ plot(sens) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?  plot(sens,\"time\") #> Warning: Use of `plotdata$value` is discouraged. #> ℹ Use `value` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead.  plot(sens,\"features\")  # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/PlotSensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Neural network structure sensitivity plot — PlotSensMLP","title":"Neural network structure sensitivity plot — PlotSensMLP","text":"Plot neural interpretation diagram colored sensitivities model","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/PlotSensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Neural network structure sensitivity plot — PlotSensMLP","text":"","code":"PlotSensMLP(   MLP.fit,   metric = \"mean\",   sens_neg_col = \"red\",   sens_pos_col = \"blue\",   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/PlotSensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Neural network structure sensitivity plot — PlotSensMLP","text":"MLP.fit fitted neural network model metric metric plot NID. can \"mean\" (default), \"median \"sqmean\". can metric combine raw sensitivities sens_neg_col character string indicating color negative sensitivity measure, default 'red'. passed argument neg_col plotnet sens_pos_col character string indicating color positive sensitivity measure, default 'blue'. passed argument pos_col plotnet ... additional arguments passed plotnet /SensAnalysisMLP","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/PlotSensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Neural network structure sensitivity plot — PlotSensMLP","text":"graphics object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/PlotSensMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Neural network structure sensitivity plot — PlotSensMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 100 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP NeuralSens::PlotSensMLP(nnetmod, trData = nntrData)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for the HessMLP Class — print.HessMLP","title":"Print method for the HessMLP Class — print.HessMLP","text":"Print sensitivities HessMLP object.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for the HessMLP Class — print.HessMLP","text":"","code":"# S3 method for HessMLP print(x, n = 5, round_digits = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for the HessMLP Class — print.HessMLP","text":"x HessMLP object created HessianMLP n integer specifying number sensitivities print per output round_digits integer number decimal places, default NULL ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.HessMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method for the HessMLP Class — print.HessMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP sens <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) sens #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #> , , 1 #>  #>            WD     TEMP #> WD   1.028311 1.174910 #> TEMP 1.174910 3.386778 #>  #> , , 2 #>  #>             WD     TEMP #> WD   -2.693044 0.856781 #> TEMP  0.856781 1.608223 #>  #> , , 3 #>  #>             WD      TEMP #> WD   -3.523861 -2.061425 #> TEMP -2.061425 -5.352261 #>  #> , , 4 #>  #>             WD      TEMP #> WD   -3.823715 -2.719650 #> TEMP -2.719650 -7.312955 #>  #> , , 5 #>  #>              WD      TEMP #> WD   -2.1583991 0.4679963 #> TEMP  0.4679963 1.9635852 #>"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for the SensMLP Class — print.SensMLP","title":"Print method for the SensMLP Class — print.SensMLP","text":"Print sensitivities SensMLP object.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for the SensMLP Class — print.SensMLP","text":"","code":"# S3 method for SensMLP print(x, n = 5, round_digits = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for the SensMLP Class — print.SensMLP","text":"x SensMLP object created SensAnalysisMLP n integer specifying number sensitivities print per output round_digits integer number decimal places, default NULL ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Print method for the SensMLP Class — print.SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.SensMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method for the SensMLP Class — print.SensMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) sens #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #>             WD      TEMP #> [1,] 0.6120725 0.8274999 #> [2,] 4.7762243 4.8944193 #> [3,] 3.6045326 4.4670914 #> [4,] 3.0769395 3.3320326 #> [5,] 3.7486139 4.9043203"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method of the summary HessMLP Class — print.summary.HessMLP","title":"Print method of the summary HessMLP Class — print.summary.HessMLP","text":"Print sensitivity metrics HessMLP object. metrics mean sensitivity, standard deviation sensitivities mean sensitivities square","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method of the summary HessMLP Class — print.summary.HessMLP","text":"","code":"# S3 method for summary.HessMLP print(x, round_digits = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method of the summary HessMLP Class — print.summary.HessMLP","text":"x summary.HessMLP object created summary method HessMLP object round_digits integer number decimal places, default NULL ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.HessMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method of the summary HessMLP Class — print.summary.HessMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP sens <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) print(summary(sens)) #> Hessian matrix of 2-5-1 MLP network. #>  #> Hessian measures of each output: #> $.outcome #> $.outcome$mean #>               WD        TEMP #> WD   -0.75371769 -0.02008632 #> TEMP -0.02008632  1.18930246 #>  #> $.outcome$std #>            WD     TEMP #> WD   2.414398 1.868676 #> TEMP 1.868676 6.269440 #>  #> $.outcome$meanSensSQ #>            WD     TEMP #> WD   2.528728 1.868313 #> TEMP 1.868313 6.379692 #>  #>  #> $.outcome #> $.outcome$mean #>               WD        TEMP #> WD   -0.75371769 -0.02008632 #> TEMP -0.02008632  1.18930246 #>  #> $.outcome$std #>            WD     TEMP #> WD   2.414398 1.868676 #> TEMP 1.868676 6.269440 #>  #> $.outcome$meanSensSQ #>            WD     TEMP #> WD   2.528728 1.868313 #> TEMP 1.868313 6.379692 #>  #>"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method of the summary SensMLP Class — print.summary.SensMLP","title":"Print method of the summary SensMLP Class — print.summary.SensMLP","text":"Print sensitivity metrics SensMLP object. metrics mean sensitivity, standard deviation sensitivities mean sensitivities square","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method of the summary SensMLP Class — print.summary.SensMLP","text":"","code":"# S3 method for summary.SensMLP print(x, round_digits = NULL, boot.alpha = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method of the summary SensMLP Class — print.summary.SensMLP","text":"x summary.SensMLP object created summary method SensMLP object round_digits integer number decimal places, default NULL boot.alpha float significance level show statistical metrics. NULL, boot.alpha inherits x used. Defaults NULL. ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Print method of the summary SensMLP Class — print.summary.SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/print.summary.SensMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print method of the summary SensMLP Class — print.summary.SensMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) print(summary(sens)) #> Sensitivity analysis of 2-5-1 MLP network. #>  #> Sensitivity measures of each output: #> $.outcome #>           mean      std meanSensSQ #> WD    2.709330 1.182700   2.956103 #> TEMP -1.520092 4.172399   4.439684 #>  #> $.outcome #>           mean      std meanSensSQ #> WD    2.709330 1.182700   2.956103 #> TEMP -1.520092 4.172399   4.439684 #>"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity of MLP models — SensAnalysisMLP","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"Function evaluating sensitivities inputs   variables mlp model","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"","code":"SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for default SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc = NULL,   deractfunc = NULL,   preProc = NULL,   terms = NULL,   output_name = NULL,   ... )  # S3 method for train SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   boot.R = NULL,   boot.seed = 1,   boot.alpha = 0.05,   ... )  # S3 method for H2OMultinomialModel SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for H2ORegressionModel SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for list SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc,   ... )  # S3 method for mlp SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nn SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nnet SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   preProc = NULL,   terms = NULL,   ... )  # S3 method for nnetar SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   ... )  # S3 method for numeric SensAnalysisMLP(   MLP.fit,   .returnSens = TRUE,   plot = TRUE,   .rawSens = FALSE,   sens_origin_layer = 1,   sens_end_layer = \"last\",   sens_origin_input = TRUE,   sens_end_input = FALSE,   trData,   actfunc = NULL,   preProc = NULL,   terms = NULL,   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"MLP.fit fitted neural network model .returnSens DEPRECATED plot logical whether plot analysis. default TRUE. .rawSens DEPRECATED sens_origin_layer numeric specifies layer neurons respect derivative must calculated. input layer specified 1 (default). sens_end_layer numeric specifies layer neurons derivative calculated. may also 'last' specify output layer (default). sens_origin_input logical specifies derivative must calculated respect inputs (TRUE) output (FALSE) sens_origin_layer layer model. default TRUE. sens_end_input logical specifies derivative calculated output (FALSE) input (TRUE) sens_end_layer layer model. default FALSE. ... additional arguments passed methods trData data.frame containing data evaluate sensitivity model actfunc character vector indicating activation function neurons layer. deractfunc character vector indicating derivative activation function neurons layer. preProc preProcess structure applied training data. See also preProcess terms function applied training data create factors. See also train output_name character name output variable order avoid changing name output variable trData '.outcome' boot.R int Number bootstrap samples calculate. Used detect significant inputs significant non-linearities. available train objects. Defaults NULL. boot.seed int Seed bootstrap evaluations. boot.alpha float Significance level statistical test.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"SensMLP object sensitivity metrics sensitivities MLP model passed function.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"case using input class factor package   need enter input data matrix, dummies must created   training neural network. , training data must given function using   trData argument.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"plots","dir":"Reference","previous_headings":"","what":"Plots","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"Plot 1: colorful plot classification   classes 2D map Plot 2: b/w plot probability   chosen class 2D map Plot 3: plot stats::predictions   data provided","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensAnalysisMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity of MLP models — SensAnalysisMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 100 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?  #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #>             WD      TEMP #> [1,] 0.6120725 0.8274999 #> [2,] 4.7762243 4.8944193 #> [3,] 3.6045326 4.4670914 #> [4,] 3.0769395 3.3320326 #> [5,] 3.7486139 4.9043203 # \\donttest{ # Try SensAnalysisMLP to calculate sensitivities with respect to output of hidden neurones NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData,                              sens_origin_layer = 2,                              sens_end_layer = \"last\",                              sens_origin_input = FALSE,                              sens_end_input = FALSE) #> Warning: All aesthetics have length 1, but the data has 5 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: Computation failed in `stat_density()`. #> Caused by error in `optimize()`: #> ! 'xmin' not less than 'xmax'  #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #>      Neuron 2.1 Neuron 2.2 Neuron 2.3 Neuron 2.4 Neuron 2.5 #> [1,]  -7.059173   6.222473   3.443051   2.967425   2.773766 #> [2,]  -7.059173   6.222473   3.443051   2.967425   2.773766 #> [3,]  -7.059173   6.222473   3.443051   2.967425   2.773766 #> [4,]  -7.059173   6.222473   3.443051   2.967425   2.773766 #> [5,]  -7.059173   6.222473   3.443051   2.967425   2.773766 ## Train caret NNET ------------------------------------------------------------ # Create trainControl ctrl_tune <- caret::trainControl(method = \"boot\",                                  savePredictions = FALSE,                                  summaryFunction = caret::defaultSummary) set.seed(150) #For replication caretmod <- caret::train(form = DEM~.,                               data = fdata.Reg.tr,                               method = \"nnet\",                               linout = TRUE,                               tuneGrid = data.frame(size = 3,                                                     decay = decay),                               maxit = iters,                               preProcess = c(\"center\",\"scale\"),                               trControl = ctrl_tune,                               metric = \"RMSE\") #> # weights:  13 #> initial  value 448.296994  #> iter  10 value 4.240464 #> iter  20 value 2.955813 #> iter  30 value 2.432667 #> iter  40 value 2.152339 #> iter  50 value 2.119534 #> iter  60 value 2.116452 #> iter  70 value 2.115042 #> final  value 2.115024  #> converged #> # weights:  13 #> initial  value 259.256650  #> iter  10 value 5.626711 #> iter  20 value 3.294659 #> iter  30 value 2.713958 #> iter  40 value 2.218022 #> iter  50 value 2.141379 #> iter  60 value 2.100764 #> iter  70 value 2.080285 #> iter  80 value 2.075813 #> iter  90 value 2.075489 #> final  value 2.075479  #> converged #> # weights:  13 #> initial  value 839.080081  #> iter  10 value 7.558987 #> iter  20 value 5.177089 #> iter  30 value 4.017037 #> iter  40 value 2.214439 #> iter  50 value 2.059848 #> iter  60 value 2.054339 #> iter  70 value 2.045965 #> iter  80 value 2.039738 #> iter  90 value 2.015979 #> iter 100 value 1.990266 #> final  value 1.990266  #> stopped after 100 iterations #> # weights:  13 #> initial  value 2367.468092  #> iter  10 value 18.231718 #> iter  20 value 6.058120 #> iter  30 value 3.242985 #> iter  40 value 2.187840 #> iter  50 value 2.071775 #> iter  60 value 2.050664 #> iter  70 value 2.028564 #> iter  80 value 2.004616 #> iter  90 value 1.984345 #> iter 100 value 1.979182 #> final  value 1.979182  #> stopped after 100 iterations #> # weights:  13 #> initial  value 466.520607  #> iter  10 value 26.340684 #> iter  20 value 5.710540 #> iter  30 value 4.505050 #> iter  40 value 3.920448 #> iter  50 value 2.611841 #> iter  60 value 2.175444 #> iter  70 value 2.114173 #> iter  80 value 2.113462 #> iter  90 value 2.113367 #> final  value 2.113363  #> converged #> # weights:  13 #> initial  value 997.898260  #> iter  10 value 7.946815 #> iter  20 value 3.796169 #> iter  30 value 2.732974 #> iter  40 value 2.185516 #> iter  50 value 2.016426 #> iter  60 value 1.996298 #> iter  70 value 1.984364 #> iter  80 value 1.983086 #> iter  90 value 1.983051 #> final  value 1.983051  #> converged #> # weights:  13 #> initial  value 926.945659  #> iter  10 value 8.424427 #> iter  20 value 3.640577 #> iter  30 value 2.446188 #> iter  40 value 2.280445 #> iter  50 value 2.270542 #> iter  60 value 2.265967 #> iter  70 value 2.255105 #> iter  80 value 2.247681 #> iter  90 value 2.244869 #> iter 100 value 2.244534 #> final  value 2.244534  #> stopped after 100 iterations #> # weights:  13 #> initial  value 142.892088  #> iter  10 value 6.910998 #> iter  20 value 5.724961 #> iter  30 value 4.789805 #> iter  40 value 3.951995 #> iter  50 value 2.271814 #> iter  60 value 2.143636 #> iter  70 value 2.081638 #> iter  80 value 2.053068 #> iter  90 value 2.050443 #> iter 100 value 2.050366 #> final  value 2.050366  #> stopped after 100 iterations #> # weights:  13 #> initial  value 224.855827  #> iter  10 value 6.463371 #> iter  20 value 4.498606 #> iter  30 value 4.138860 #> iter  40 value 2.669166 #> iter  50 value 2.190129 #> iter  60 value 2.057145 #> iter  70 value 2.031552 #> iter  80 value 2.030720 #> iter  90 value 2.030692 #> final  value 2.030691  #> converged #> # weights:  13 #> initial  value 56.501349  #> iter  10 value 4.546989 #> iter  20 value 2.412030 #> iter  30 value 2.211975 #> iter  40 value 2.126688 #> iter  50 value 2.118982 #> iter  60 value 2.118522 #> final  value 2.118318  #> converged #> # weights:  13 #> initial  value 1537.541660  #> iter  10 value 45.893358 #> iter  20 value 14.179474 #> iter  30 value 6.293025 #> iter  40 value 2.966490 #> iter  50 value 2.250200 #> iter  60 value 2.200879 #> iter  70 value 2.132716 #> iter  80 value 2.116182 #> iter  90 value 2.113110 #> final  value 2.112921  #> converged #> # weights:  13 #> initial  value 3354.677303  #> iter  10 value 8.207666 #> iter  20 value 5.979884 #> iter  30 value 4.402456 #> iter  40 value 2.680091 #> iter  50 value 2.140337 #> iter  60 value 2.120667 #> iter  70 value 2.090521 #> iter  80 value 2.072754 #> iter  90 value 2.064807 #> iter 100 value 2.063714 #> final  value 2.063714  #> stopped after 100 iterations #> # weights:  13 #> initial  value 1495.824272  #> iter  10 value 6.518061 #> iter  20 value 4.118493 #> iter  30 value 3.189918 #> iter  40 value 2.385578 #> iter  50 value 2.157477 #> iter  60 value 2.139096 #> iter  70 value 2.136309 #> final  value 2.136306  #> converged #> # weights:  13 #> initial  value 11.757653  #> iter  10 value 3.909688 #> iter  20 value 2.556194 #> iter  30 value 2.363998 #> iter  40 value 2.217477 #> iter  50 value 2.190203 #> iter  60 value 2.185572 #> iter  70 value 2.183208 #> final  value 2.183205  #> converged #> # weights:  13 #> initial  value 154.922979  #> iter  10 value 5.007005 #> iter  20 value 2.986467 #> iter  30 value 2.324922 #> iter  40 value 2.096300 #> iter  50 value 2.077648 #> iter  60 value 2.071380 #> iter  70 value 2.070058 #> final  value 2.070057  #> converged #> # weights:  13 #> initial  value 1295.102618  #> iter  10 value 20.587942 #> iter  20 value 5.648115 #> iter  30 value 4.498304 #> iter  40 value 2.651133 #> iter  50 value 2.284889 #> iter  60 value 2.195444 #> iter  70 value 2.167062 #> iter  80 value 2.151623 #> iter  90 value 2.147625 #> final  value 2.147598  #> converged #> # weights:  13 #> initial  value 6196.422374  #> iter  10 value 9.791647 #> iter  20 value 8.194195 #> iter  30 value 5.683012 #> iter  40 value 2.600464 #> iter  50 value 2.259054 #> iter  60 value 2.197961 #> iter  70 value 2.156120 #> iter  80 value 2.116732 #> iter  90 value 2.099782 #> iter 100 value 2.096804 #> final  value 2.096804  #> stopped after 100 iterations #> # weights:  13 #> initial  value 165.586684  #> iter  10 value 6.965082 #> iter  20 value 2.942559 #> iter  30 value 2.400000 #> iter  40 value 2.192173 #> iter  50 value 2.147873 #> iter  60 value 2.145626 #> iter  70 value 2.144777 #> final  value 2.144775  #> converged #> # weights:  13 #> initial  value 3685.490536  #> iter  10 value 7.497643 #> iter  20 value 4.678847 #> iter  30 value 3.198315 #> iter  40 value 2.332407 #> iter  50 value 2.266814 #> iter  60 value 2.246946 #> iter  70 value 2.230340 #> iter  80 value 2.225874 #> iter  90 value 2.223878 #> final  value 2.223678  #> converged #> # weights:  13 #> initial  value 1883.675021  #> iter  10 value 9.414955 #> iter  20 value 6.244387 #> iter  30 value 3.029821 #> iter  40 value 2.441478 #> iter  50 value 2.388556 #> iter  60 value 2.310839 #> iter  70 value 2.223355 #> iter  80 value 2.200783 #> iter  90 value 2.161414 #> iter 100 value 2.149199 #> final  value 2.149199  #> stopped after 100 iterations #> # weights:  13 #> initial  value 2896.671370  #> iter  10 value 5.948826 #> iter  20 value 3.887106 #> iter  30 value 2.866152 #> iter  40 value 2.358066 #> iter  50 value 2.269247 #> iter  60 value 2.244193 #> iter  70 value 2.236110 #> iter  80 value 2.235692 #> final  value 2.235688  #> converged #> # weights:  13 #> initial  value 1160.170910  #> iter  10 value 31.153142 #> iter  20 value 11.439016 #> iter  30 value 5.790787 #> iter  40 value 3.100087 #> iter  50 value 2.444872 #> iter  60 value 2.285386 #> iter  70 value 2.154482 #> iter  80 value 2.141414 #> iter  90 value 2.136232 #> final  value 2.135919  #> converged #> # weights:  13 #> initial  value 3189.626236  #> iter  10 value 5.430796 #> iter  20 value 3.625953 #> iter  30 value 2.730844 #> iter  40 value 2.340266 #> iter  50 value 2.247853 #> iter  60 value 2.174153 #> iter  70 value 2.141220 #> iter  80 value 2.127328 #> iter  90 value 2.098656 #> iter 100 value 2.071428 #> final  value 2.071428  #> stopped after 100 iterations #> # weights:  13 #> initial  value 316.924838  #> iter  10 value 5.937193 #> iter  20 value 4.755514 #> iter  30 value 3.663735 #> iter  40 value 2.233143 #> iter  50 value 2.134806 #> iter  60 value 2.113021 #> iter  70 value 2.101975 #> iter  80 value 2.100327 #> iter  90 value 2.100020 #> final  value 2.100013  #> converged #> # weights:  13 #> initial  value 1475.824755  #> iter  10 value 6.897328 #> iter  20 value 5.017823 #> iter  30 value 3.550261 #> iter  40 value 2.277808 #> iter  50 value 2.126565 #> iter  60 value 2.103660 #> iter  70 value 2.089784 #> iter  80 value 2.089397 #> final  value 2.089397  #> converged #> # weights:  13 #> initial  value 2705.392001  #> iter  10 value 25.983461 #> iter  20 value 9.945895 #> iter  30 value 4.266584 #> iter  40 value 3.251963 #> iter  50 value 2.661341 #> iter  60 value 2.218021 #> iter  70 value 2.117866 #> iter  80 value 2.104051 #> iter  90 value 2.101507 #> final  value 2.101324  #> converged  # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(caretmod) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-3-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $.outcome #>              WD       TEMP #> [1,] 0.05494945 0.05477037 #> [2,] 0.06038813 0.05438979 #> [3,] 0.05904747 0.06004020 #> [4,] 0.05927444 0.06659510 #> [5,] 0.05834915 0.04605626  ## Train h2o NNET -------------------------------------------------------------- # Create a cluster with 4 available cores h2o::h2o.init(ip = \"localhost\",               nthreads = 4) #>  #> H2O is not running yet, starting it now... #>  #> Note:  In case of errors look at the following log files: #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file165047a66f44/h2o_jpizarroso_started_from_r.out #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file16506dba4019/h2o_jpizarroso_started_from_r.err #>  #>  #> Starting H2O JVM and connecting:  Connection successful! #>  #> R is connected to the H2O cluster:  #>     H2O cluster uptime:         2 seconds 810 milliseconds  #>     H2O cluster timezone:       Europe/Madrid  #>     H2O data parsing timezone:  UTC  #>     H2O cluster version:        3.44.0.3  #>     H2O cluster version age:    6 months and 4 days  #>     H2O cluster name:           H2O_started_from_R_jpizarroso_inw091  #>     H2O cluster total nodes:    1  #>     H2O cluster total memory:   7.91 GB  #>     H2O cluster total cores:    20  #>     H2O cluster allowed cores:  4  #>     H2O cluster healthy:        TRUE  #>     H2O Connection ip:          localhost  #>     H2O Connection port:        54321  #>     H2O Connection proxy:       NA  #>     H2O Internal Security:      FALSE  #>     R Version:                  R version 4.3.3 (2024-02-29 ucrt)  #> Warning:  #> Your H2O cluster version is (6 months and 4 days) old. There may be a newer version available. #> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html #>   # Reset the cluster h2o::h2o.removeAll() fdata_h2o <- h2o::as.h2o(x = fdata.Reg.tr, destination_frame = \"fdata_h2o\") #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  set.seed(150) h2omod <-h2o:: h2o.deeplearning(x = names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)],                                      y = names(fdata.Reg.tr)[1],                                      distribution = \"AUTO\",                                      training_frame = fdata_h2o,                                      standardize = TRUE,                                      activation = \"Tanh\",                                      hidden = c(hidden_neurons),                                      stopping_rounds = 0,                                      epochs = iters,                                      seed = 150,                                      model_id = \"nnet_h2o\",                                      adaptive_rate = FALSE,                                      rate_decay = decay,                                      export_weights_and_biases = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(h2omod) #> Error in value[[3L]](cond): The training data has not been detected, load the data to the h2o cluster  # Turn off the cluster h2o::h2o.shutdown(prompt = FALSE) rm(fdata_h2o)  ## Train RSNNS NNET ------------------------------------------------------------ # Normalize data using RSNNS algorithms trData <- as.data.frame(RSNNS::normalizeData(fdata.Reg.tr)) names(trData) <- names(fdata.Reg.tr) set.seed(150) RSNNSmod <-RSNNS::mlp(x = trData[,2:ncol(trData)],                            y = trData[,1],                            size = hidden_neurons,                            linOut = TRUE,                            learnFuncParams=c(decay),                            maxit=iters)  # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(RSNNSmod, trData = trData, output_name = \"DEM\") #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?  #> Sensitivity analysis of 2-5-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $DEM #>             WD      TEMP #> [1,] 0.6822845 1.0399880 #> [2,] 0.6925971 1.0693525 #> [3,] 0.4945469 1.0524738 #> [4,] 0.4650314 0.8402342 #> [5,] 0.4720409 0.9847080  ## USE DEFAULT METHOD ---------------------------------------------------------- NeuralSens::SensAnalysisMLP(caretmod$finalModel$wts,                             trData = fdata.Reg.tr,                             mlpstr = caretmod$finalModel$n,                             coefnames = caretmod$coefnames,                             actfun = c(\"linear\",\"sigmoid\",\"linear\"),                             output_name = \"DEM\") #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?  #> Sensitivity analysis of 2-3-1 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $DEM #>              WD       TEMP #> [1,] 0.06229568 0.05690481 #> [2,] 0.06133223 0.05872422 #> [3,] 0.06134104 0.05680759 #> [4,] 0.06159004 0.05322817 #> [5,] 0.06092965 0.06176240  ################################################################################ #########################  CLASSIFICATION NNET ################################# ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.cl <- fdata[,2:ncol(fdata)] fdata.Reg.cl[,2:3] <- fdata.Reg.cl[,2:3]/10 fdata.Reg.cl[,1] <- fdata.Reg.cl[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.cl, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.cl)  # Factorize the output fdata.Reg.cl$DEM <- factor(round(fdata.Reg.cl$DEM, digits = 1))  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.cl, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.cl)  ## Train caret NNET ------------------------------------------------------------ # Create trainControl ctrl_tune <- caret::trainControl(method = \"boot\",                                  savePredictions = FALSE,                                  summaryFunction = caret::defaultSummary) set.seed(150) #For replication caretmod <- caret::train(form = DEM~.,                                 data = fdata.Reg.cl,                                 method = \"nnet\",                                 linout = FALSE,                                 tuneGrid = data.frame(size = hidden_neurons,                                                       decay = decay),                                 maxit = iters,                                 preProcess = c(\"center\",\"scale\"),                                 trControl = ctrl_tune,                                 metric = \"Accuracy\") #> # weights:  45 #> initial  value 3772.842618  #> iter  10 value 1557.564302 #> iter  20 value 1214.199134 #> iter  30 value 1110.296648 #> iter  40 value 1070.982383 #> iter  50 value 1040.282432 #> iter  60 value 1026.243143 #> iter  70 value 1022.185164 #> iter  80 value 1018.932608 #> iter  90 value 1014.806386 #> iter 100 value 1012.499658 #> final  value 1012.499658  #> stopped after 100 iterations #> # weights:  45 #> initial  value 2668.172951  #> iter  10 value 1496.742417 #> iter  20 value 1187.526898 #> iter  30 value 1050.272689 #> iter  40 value 1013.322223 #> iter  50 value 996.639753 #> iter  60 value 979.316887 #> iter  70 value 969.798307 #> iter  80 value 966.613930 #> iter  90 value 965.693488 #> iter 100 value 965.463123 #> final  value 965.463123  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3297.203342  #> iter  10 value 1377.287449 #> iter  20 value 1112.310755 #> iter  30 value 1023.822455 #> iter  40 value 991.843628 #> iter  50 value 956.727170 #> iter  60 value 941.910334 #> iter  70 value 936.609226 #> iter  80 value 935.410703 #> iter  90 value 935.163698 #> iter 100 value 935.127337 #> final  value 935.127337  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3568.399296  #> iter  10 value 1489.763894 #> iter  20 value 1219.257218 #> iter  30 value 1076.771253 #> iter  40 value 1024.503423 #> iter  50 value 998.708032 #> iter  60 value 981.840999 #> iter  70 value 973.720379 #> iter  80 value 971.090791 #> iter  90 value 968.967523 #> iter 100 value 968.494390 #> final  value 968.494390  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3003.535668  #> iter  10 value 1439.044442 #> iter  20 value 1176.600696 #> iter  30 value 1048.481267 #> iter  40 value 1016.572547 #> iter  50 value 1011.297145 #> iter  60 value 999.404464 #> iter  70 value 988.451143 #> iter  80 value 977.282991 #> iter  90 value 972.906275 #> iter 100 value 971.896597 #> final  value 971.896597  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3408.995747  #> iter  10 value 1341.622904 #> iter  20 value 1089.969900 #> iter  30 value 1029.207268 #> iter  40 value 996.486089 #> iter  50 value 988.952619 #> iter  60 value 976.464472 #> iter  70 value 972.214434 #> iter  80 value 970.881761 #> iter  90 value 969.641644 #> iter 100 value 969.485114 #> final  value 969.485114  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4281.102149  #> iter  10 value 1513.066785 #> iter  20 value 1329.056483 #> iter  30 value 1209.982389 #> iter  40 value 1118.764207 #> iter  50 value 1076.036468 #> iter  60 value 1061.718419 #> iter  70 value 1050.693937 #> iter  80 value 1040.872146 #> iter  90 value 1029.507488 #> iter 100 value 1025.374541 #> final  value 1025.374541  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3265.828022  #> iter  10 value 1456.434584 #> iter  20 value 1166.808947 #> iter  30 value 1057.977863 #> iter  40 value 1008.996008 #> iter  50 value 978.114751 #> iter  60 value 970.985521 #> iter  70 value 966.104095 #> iter  80 value 963.006874 #> iter  90 value 962.094607 #> iter 100 value 961.841612 #> final  value 961.841612  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3019.717560  #> iter  10 value 1333.523881 #> iter  20 value 1154.064664 #> iter  30 value 1067.529755 #> iter  40 value 1054.332888 #> iter  50 value 1049.493394 #> iter  60 value 1042.420751 #> iter  70 value 1025.594053 #> iter  80 value 1018.729658 #> iter  90 value 1016.946541 #> iter 100 value 1015.397734 #> final  value 1015.397734  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3405.760894  #> iter  10 value 1597.431168 #> iter  20 value 1263.793552 #> iter  30 value 1114.159495 #> iter  40 value 1048.729401 #> iter  50 value 1023.871076 #> iter  60 value 976.998748 #> iter  70 value 960.770263 #> iter  80 value 954.884250 #> iter  90 value 952.488444 #> iter 100 value 951.344477 #> final  value 951.344477  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3013.688100  #> iter  10 value 1535.862594 #> iter  20 value 1231.453772 #> iter  30 value 1132.096002 #> iter  40 value 1106.684078 #> iter  50 value 1083.676956 #> iter  60 value 1041.694085 #> iter  70 value 1032.125938 #> iter  80 value 1022.027396 #> iter  90 value 1006.266589 #> iter 100 value 1000.802958 #> final  value 1000.802958  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4628.455991  #> iter  10 value 1552.025703 #> iter  20 value 1211.906415 #> iter  30 value 1084.177292 #> iter  40 value 1064.063606 #> iter  50 value 1055.265752 #> iter  60 value 1049.279896 #> iter  70 value 1040.272607 #> iter  80 value 1038.639372 #> iter  90 value 1038.099537 #> iter 100 value 1037.787531 #> final  value 1037.787531  #> stopped after 100 iterations #> # weights:  45 #> initial  value 2860.694342  #> iter  10 value 1511.390857 #> iter  20 value 1168.419352 #> iter  30 value 1094.111978 #> iter  40 value 1073.168803 #> iter  50 value 1069.749263 #> iter  60 value 1066.180221 #> iter  70 value 1064.629158 #> iter  80 value 1064.226117 #> iter  90 value 1063.853095 #> iter 100 value 1062.896671 #> final  value 1062.896671  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3377.433754  #> iter  10 value 1446.109214 #> iter  20 value 1171.013957 #> iter  30 value 1114.422290 #> iter  40 value 1085.906251 #> iter  50 value 1056.654790 #> iter  60 value 1043.204252 #> iter  70 value 1039.982529 #> iter  80 value 1037.244251 #> iter  90 value 1036.464530 #> iter 100 value 1036.224249 #> final  value 1036.224249  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3253.715832  #> iter  10 value 1411.560701 #> iter  20 value 1173.605210 #> iter  30 value 1141.598323 #> iter  40 value 1110.027489 #> iter  50 value 1075.678228 #> iter  60 value 1065.287014 #> iter  70 value 1061.607689 #> iter  80 value 1054.627824 #> iter  90 value 1034.673250 #> iter 100 value 1029.300197 #> final  value 1029.300197  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3863.665785  #> iter  10 value 1387.778891 #> iter  20 value 1141.991241 #> iter  30 value 1113.544623 #> iter  40 value 1093.112775 #> iter  50 value 1064.466827 #> iter  60 value 1053.501083 #> iter  70 value 1035.802924 #> iter  80 value 1020.443915 #> iter  90 value 1015.693675 #> iter 100 value 1014.111725 #> final  value 1014.111725  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3378.665527  #> iter  10 value 1400.194414 #> iter  20 value 1134.238327 #> iter  30 value 1072.434905 #> iter  40 value 1022.352730 #> iter  50 value 1009.326061 #> iter  60 value 1003.254602 #> iter  70 value 998.622585 #> iter  80 value 996.924622 #> iter  90 value 995.431517 #> iter 100 value 995.296204 #> final  value 995.296204  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3778.883814  #> iter  10 value 1287.189554 #> iter  20 value 1085.911856 #> iter  30 value 1048.258887 #> iter  40 value 1031.285258 #> iter  50 value 1020.746772 #> iter  60 value 1009.982512 #> iter  70 value 1001.117193 #> iter  80 value 996.249854 #> iter  90 value 995.002293 #> iter 100 value 994.464160 #> final  value 994.464160  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3120.123677  #> iter  10 value 1354.765028 #> iter  20 value 1127.326225 #> iter  30 value 1030.192879 #> iter  40 value 995.812629 #> iter  50 value 987.332787 #> iter  60 value 980.506578 #> iter  70 value 976.439833 #> iter  80 value 975.171226 #> iter  90 value 974.762849 #> iter 100 value 974.669379 #> final  value 974.669379  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4498.743913  #> iter  10 value 1444.335556 #> iter  20 value 1196.519410 #> iter  30 value 1099.018244 #> iter  40 value 1029.077344 #> iter  50 value 1005.258084 #> iter  60 value 997.634916 #> iter  70 value 996.394696 #> iter  80 value 995.712015 #> iter  90 value 995.645881 #> iter 100 value 995.634593 #> final  value 995.634593  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4178.969476  #> iter  10 value 1484.773196 #> iter  20 value 1236.558325 #> iter  30 value 1131.376400 #> iter  40 value 1058.462683 #> iter  50 value 1018.919757 #> iter  60 value 1006.929988 #> iter  70 value 1000.924164 #> iter  80 value 997.057751 #> iter  90 value 992.146696 #> iter 100 value 990.467451 #> final  value 990.467451  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3763.027227  #> iter  10 value 1502.580970 #> iter  20 value 1286.900665 #> iter  30 value 1169.576206 #> iter  40 value 1122.939531 #> iter  50 value 1108.972049 #> iter  60 value 1104.953532 #> iter  70 value 1103.270073 #> iter  80 value 1101.645238 #> iter  90 value 1098.946203 #> iter 100 value 1095.719635 #> final  value 1095.719635  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3634.283962  #> iter  10 value 1393.174055 #> iter  20 value 1064.065558 #> iter  30 value 998.290332 #> iter  40 value 973.809987 #> iter  50 value 963.302264 #> iter  60 value 949.245382 #> iter  70 value 943.582074 #> iter  80 value 942.712725 #> iter  90 value 942.090755 #> iter 100 value 941.998670 #> final  value 941.998670  #> stopped after 100 iterations #> # weights:  45 #> initial  value 4122.006202  #> iter  10 value 1440.624472 #> iter  20 value 1201.124592 #> iter  30 value 1081.368683 #> iter  40 value 1019.710036 #> iter  50 value 998.220181 #> iter  60 value 991.313592 #> iter  70 value 988.455638 #> iter  80 value 986.674128 #> iter  90 value 985.122136 #> iter 100 value 984.455611 #> final  value 984.455611  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3169.917474  #> iter  10 value 1708.163015 #> iter  20 value 1275.618128 #> iter  30 value 1086.379758 #> iter  40 value 1017.730370 #> iter  50 value 994.236230 #> iter  60 value 984.400401 #> iter  70 value 982.537214 #> iter  80 value 981.070886 #> iter  90 value 980.488255 #> iter 100 value 980.416752 #> final  value 980.416752  #> stopped after 100 iterations #> # weights:  45 #> initial  value 3805.876854  #> iter  10 value 1440.491291 #> iter  20 value 1164.498683 #> iter  30 value 1068.693224 #> iter  40 value 1030.794446 #> iter  50 value 1012.456002 #> iter  60 value 1005.801464 #> iter  70 value 1003.516310 #> iter  80 value 1002.997394 #> iter  90 value 1002.812312 #> iter 100 value 1002.764592 #> final  value 1002.764592  #> stopped after 100 iterations  # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(caretmod) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: minimum occurred at one end of the range  #> Sensitivity analysis of 2-5-5 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $X0.6 #>             WD      TEMP #> [1,] -3.090643 -3.244879 #> [2,] -2.014266  2.340029 #> [3,] -2.301017  3.314643 #> [4,] -1.646404  2.363488 #> [5,] -3.194331  4.590329 #> $X0.7 #>             WD      TEMP #> [1,]  1.304236  1.328709 #> [2,] -4.758324 -5.036435 #> [3,] -3.562429 -2.846620 #> [4,] -2.543834 -1.934337 #> [5,] -4.986845 -3.618596 #> $X0.8 #>              WD       TEMP #> [1,]  3.2431659  3.4280643 #> [2,]  0.9767421 -1.0853447 #> [3,]  0.3783996 -1.6561027 #> [4,] -0.1020680 -0.9648333 #> [5,]  0.7293068 -2.3794496 #> $X0.5 #>            WD     TEMP #> [1,] 1.318590 1.452776 #> [2,] 4.054048 4.764898 #> [3,] 2.825830 2.851447 #> [4,] 1.954111 1.987588 #> [5,] 4.062261 3.559360 #> $X0.9 #>             WD       TEMP #> [1,] -2.776717 -2.9660789 #> [2,]  1.741819 -0.9797513 #> [3,]  2.658557 -1.6599594 #> [4,]  2.337693 -1.4494932 #> [5,]  3.388717 -2.1470112  ## Train h2o NNET -------------------------------------------------------------- # Create local cluster with 4 available cores h2o::h2o.init(ip = \"localhost\",               nthreads = 4) #>  #> H2O is not running yet, starting it now... #>  #> Note:  In case of errors look at the following log files: #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file16505076768e/h2o_jpizarroso_started_from_r.out #>     C:\\Users\\JPIZAR~1\\AppData\\Local\\Temp\\RtmpmwWafn\\file16502bc75ecb/h2o_jpizarroso_started_from_r.err #>  #>  #> Starting H2O JVM and connecting:  Connection successful! #>  #> R is connected to the H2O cluster:  #>     H2O cluster uptime:         2 seconds 781 milliseconds  #>     H2O cluster timezone:       Europe/Madrid  #>     H2O data parsing timezone:  UTC  #>     H2O cluster version:        3.44.0.3  #>     H2O cluster version age:    6 months and 4 days  #>     H2O cluster name:           H2O_started_from_R_jpizarroso_vno719  #>     H2O cluster total nodes:    1  #>     H2O cluster total memory:   7.91 GB  #>     H2O cluster total cores:    20  #>     H2O cluster allowed cores:  4  #>     H2O cluster healthy:        TRUE  #>     H2O Connection ip:          localhost  #>     H2O Connection port:        54321  #>     H2O Connection proxy:       NA  #>     H2O Internal Security:      FALSE  #>     R Version:                  R version 4.3.3 (2024-02-29 ucrt)  #> Warning:  #> Your H2O cluster version is (6 months and 4 days) old. There may be a newer version available. #> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html #>   # Reset the cluster h2o::h2o.removeAll() fdata_h2o <- h2o::as.h2o(x = fdata.Reg.cl, destination_frame = \"fdata_h2o\") #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  set.seed(150) h2omod <- h2o::h2o.deeplearning(x = names(fdata.Reg.cl)[2:ncol(fdata.Reg.cl)],                                        y = names(fdata.Reg.cl)[1],                                        distribution = \"AUTO\",                                        training_frame = fdata_h2o,                                        standardize = TRUE,                                        activation = \"Tanh\",                                        hidden = c(hidden_neurons),                                        stopping_rounds = 0,                                        epochs = iters,                                        seed = 150,                                        model_id = \"nnet_h2o\",                                        adaptive_rate = FALSE,                                        rate_decay = decay,                                        export_weights_and_biases = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(h2omod) #> Error in value[[3L]](cond): The training data has not been detected, load the data to the h2o cluster  # Apaga el cluster h2o::h2o.shutdown(prompt = FALSE) rm(fdata_h2o)  ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  45 #> initial  value 3571.076907  #> iter  10 value 1391.280224 #> iter  20 value 1195.078969 #> iter  30 value 1148.503871 #> iter  40 value 1071.086906 #> iter  50 value 1016.236754 #> iter  60 value 1003.803225 #> iter  70 value 998.327935 #> iter  80 value 996.114736 #> iter  90 value 995.001087 #> iter 100 value 994.799975 #> final  value 994.799975  #> stopped after 100 iterations # Try SensAnalysisMLP NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?  #> Sensitivity analysis of 2-5-5 MLP network. #>  #>   1980 samples #>  #> Sensitivities of each output (only 5 first samples): #> $X0.6 #>             WD       TEMP #> [1,] -2.968083 -3.6639068 #> [2,] -2.139224  0.1506373 #> [3,] -1.140777  0.4886891 #> [4,] -1.048007  0.4451950 #> [5,] -1.340015  0.5999645 #> $X0.7 #>              WD        TEMP #> [1,] -0.4553591 -0.02138749 #> [2,] -4.0477280 -4.37830121 #> [3,] -2.7435740 -2.99623805 #> [4,] -1.9237215 -1.87727229 #> [5,] -3.5930227 -3.96295593 #> $X0.8 #>              WD       TEMP #> [1,]  1.9192403  3.1613092 #> [2,] -0.3239739 -1.0853134 #> [3,] -0.3781555 -1.0807294 #> [4,] -0.2235086 -0.7306592 #> [5,] -0.3940855 -1.4305374 #> $X0.5 #>            WD      TEMP #> [1,] 1.122822 0.5389337 #> [2,] 5.718809 5.5029293 #> [3,] 3.804515 3.7155578 #> [4,] 2.744626 2.2923798 #> [5,] 4.900349 4.9341007 #> $X0.9 #>             WD        TEMP #> [1,] 0.3345239 -0.10232568 #> [2,] 0.8188511 -0.15597407 #> [3,] 0.4793482 -0.09476533 #> [4,] 0.4662102 -0.10846099 #> [5,] 0.4495483 -0.09757113 # }"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensDotPlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity scatter plot against input values — SensDotPlot","title":"Sensitivity scatter plot against input values — SensDotPlot","text":"Plot sensitivities neural network output respect inputs","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensDotPlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity scatter plot against input values — SensDotPlot","text":"","code":"SensDotPlot(   object,   fdata = NULL,   input_vars = \"all\",   output_vars = \"all\",   smooth = FALSE,   nspline = NULL,   color = NULL,   grid = FALSE,   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensDotPlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity scatter plot against input values — SensDotPlot","text":"object fitted neural network model array containing raw sensitivities function SensAnalysisMLP fdata data.frame containing data evaluate sensitivity model. input_vars character vector variables create scatter plot. \"\", scatter plots created input variables fdata. output_vars character vector variables create scatter plot. \"\", scatter plots created output variables fdata. smooth logical TRUE, geom_smooth plots added variable plot nspline integer smooth TRUE, determine degree spline used perform geom_smooth. nspline NULL, square root length data used degrees spline. color character specifying name numeric variable fdata color scatter plot. grid logical. TRUE, plots created show together using arrangeGrob ... arguments passed  SensAnalysisMLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensDotPlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity scatter plot against input values — SensDotPlot","text":"list geom_point plots inputs variables representing sensitivity output respect inputs","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensDotPlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity scatter plot against input values — SensDotPlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensDotPlot NeuralSens::SensDotPlot(nnetmod, fdata = nntrData)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature sensitivity plot — SensFeaturePlot","title":"Feature sensitivity plot — SensFeaturePlot","text":"Show distribution sensitivities output geom_sina() plot color depends input values","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature sensitivity plot — SensFeaturePlot","text":"","code":"SensFeaturePlot(object, fdata = NULL, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature sensitivity plot — SensFeaturePlot","text":"object fitted neural network model array containing raw sensitivities function SensAnalysisMLP fdata data.frame containing data evaluate sensitivity model. needed raw sensitivities passed object ... arguments passed  SensAnalysisMLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature sensitivity plot — SensFeaturePlot","text":"list Feature sensitivity plot described https://www.r-bloggers.com/2019/03/-gentle-introduction--shap-values--r/","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Feature sensitivity plot — SensFeaturePlot","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensFeaturePlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature sensitivity plot — SensFeaturePlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) NeuralSens::SensFeaturePlot(sens)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot sensitivities of a neural network model — SensitivityPlots","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"Function plot sensitivities created SensAnalysisMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"","code":"SensitivityPlots(   sens = NULL,   der = TRUE,   zoom = TRUE,   quit.legend = FALSE,   output = 1,   plot_type = NULL,   inp_var = NULL,   title = \"Sensitivity Plots\",   dodge_var = FALSE )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"sens SensAnalysisMLP object created SensAnalysisMLP HessMLP object created HessianMLP. der logical indicating density plots created. default TRUE zoom logical indicating distributions zoomed tiny appreciated third plot. facet_zoom function ggforce package required. quit.legend logical indicating legend third plot removed. default FALSE output numeric character specifying output neuron output name plotted. default first output (output = 1). plot_type character indicating 3 plots show. Useful several variables analyzed. Acceptable values 'mean_sd', 'square', 'raw' corresponding first, second third plot respectively. NULL, plots shown time. default NULL. inp_var character indicating input variable show density plot. useful choosing plot_type='raw' show density plot one input variable. NULL, variables plotted density plot. default NULL. title character title sensitivity plots dodge_var bool Flag indicate x ticks meanSensSQ plot must dodge . Useful long input names.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"List following plot output: Plot 1: colorful plot   classification classes 2D map Plot 2: b/w plot   probability chosen class 2D map Plot 3: plot   stats::predictions data provided param der FALSE","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensitivityPlots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot sensitivities of a neural network model — SensitivityPlots","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) NeuralSens::SensitivityPlots(sens) #> Warning: All aesthetics have length 1, but the data has 2 rows. #> ℹ Did you mean to use `annotate()`?"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot sensitivities of a neural network model — SensMatPlot","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"Function plot sensitivities created HessianMLP.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"","code":"SensMatPlot(   hess,   sens = NULL,   output = 1,   metric = c(\"mean\", \"std\", \"meanSensSQ\"),   senstype = c(\"matrix\", \"interactions\"),   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"hess HessMLP object created HessianMLP. sens SensMLP object created SensAnalysisMLP. output numeric character specifying output neuron output name plotted. default first output (output = 1). metric character specifying metric plotted. can \"mean\", \"std\" \"meanSensSQ\". senstype character specifying type plot plotted. can \"matrix\" \"interactions\". type = \"matrix\", second derivatives plotted. type = \"interactions\" main diagonal first derivatives respect input variable. ... argument passed similar ggcorrplot arguments.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"list ggplots, one output neuron.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"code function based ggcorrplot() function package ggcorrplot. However, due inhability changing limits color scale, keeps giving warning function used color scale overwritten.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMatPlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot sensitivities of a neural network model — SensMatPlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 100 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP H <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) NeuralSens::SensMatPlot(H)  S <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) NeuralSens::SensMatPlot(H, S, senstype = \"interactions\")"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Constructor of the SensMLP Class — SensMLP","title":"Constructor of the SensMLP Class — SensMLP","text":"Create object SensMLP class","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constructor of the SensMLP Class — SensMLP","text":"","code":"SensMLP(   sens = list(),   raw_sens = list(),   mlp_struct = numeric(),   trData = data.frame(),   coefnames = character(),   output_name = character(),   cv = NULL,   boot = NULL,   boot.alpha = NULL )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constructor of the SensMLP Class — SensMLP","text":"sens list sensitivity measures, one data.frame per output neuron raw_sens list sensitivities, one matrix per output neuron mlp_struct numeric vector describing structur MLP model trData data.frame data used calculate sensitivities coefnames character vector name predictor(s) output_name character vector name output(s) cv list list critical values significance std mean square. boot array bootstrapped sensitivity measures. boot.alpha array significance level. Defaults NULL. available analyzed caret::train models.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constructor of the SensMLP Class — SensMLP","text":"SensMLP object","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Constructor of the SensMLP Class — SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity analysis plot over time of the data — SensTimePlot","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"Plot sensitivity neural network output respect inputs time variable data provided","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"","code":"SensTimePlot(   object,   fdata = NULL,   date.var = NULL,   facet = FALSE,   smooth = FALSE,   nspline = NULL,   ... )"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"object fitted neural network model array containing raw sensitivities function SensAnalysisMLP fdata data.frame containing data evaluate sensitivity model. needed raw sensitivities passed object date.var Posixct vector date sample fdata NULL, first variable Posixct format fdata used dates facet logical TRUE, function facet_grid ggplot2 used smooth logical TRUE, geom_smooth plots added variable plot nspline integer smooth TRUE, determine degree spline used perform geom_smooth. nspline NULL, square root length timeseries used degrees spline. ... arguments passed  SensAnalysisMLP function","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"list geom_line plots inputs variables representing sensitivity output respect inputs time","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/SensTimePlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity analysis plot over time of the data — SensTimePlot","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR fdata[,3] <- ifelse(as.data.frame(fdata)[,3] %in% c(\"SUN\",\"SAT\"), 0, 1) ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) #> Warning: These variables have zero variances: WD nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                       data = nntrData,                       linear.output = TRUE,                       size = hidden_neurons,                       decay = decay,                       maxit = iters) #> # weights:  21 #> initial  value 2518.847637  #> iter  10 value 1980.676972 #> iter  20 value 1863.605952 #> iter  30 value 1714.408226 #> iter  40 value 1702.860135 #> iter  50 value 1697.085071 #> iter  60 value 1696.693269 #> iter  70 value 1696.690378 #> final  value 1696.690265  #> converged # Try SensTimePlot NeuralSens::SensTimePlot(nnetmod, fdata = nntrData, date.var = NULL) #> Warning: Use of `plotdata$value` is discouraged. #> ℹ Use `value` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead. #> Warning: Use of `plotdata$variable` is discouraged. #> ℹ Use `variable` instead."},{"path":"https://jaipizgon.github.io/NeuralSens/reference/simdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data to test the package functionalities — simdata","title":"Simulated data to test the package functionalities — simdata","text":"data.frame 2000 rows 4 columns 3 input variables X1, X2, X3 one output variable Y. data already scaled, generated using following code: set.seed(150) simdata <- data.frame(    \"X1\" = rnorm(2000, 0, 1),    \"X2\" = rnorm(2000, 0, 1),    \"X3\" = rnorm(2000, 0, 1)  ) simdata$Y <- simdata$X1^2 + 0.5*simdata$X2 + 0.1*rnorm(2000, 0, 1)","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/simdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data to test the package functionalities — simdata","text":"data frame 2000 rows 4 variables: X1 Random input 1 X2 Random input 2 X3 Random input 3 Y Output","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/simdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulated data to test the package functionalities — simdata","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/simdata.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulated data to test the package functionalities — simdata","text":"Jaime Pizarroso Gonzalo","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.HessMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for the HessMLP Class — summary.HessMLP","title":"Summary Method for the HessMLP Class — summary.HessMLP","text":"Print sensitivity metrics HessMLP object. metrics mean sensitivity, standard deviation sensitivities mean sensitivities square","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.HessMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for the HessMLP Class — summary.HessMLP","text":"","code":"# S3 method for HessMLP summary(object, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.HessMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for the HessMLP Class — summary.HessMLP","text":"object HessMLP object created HessianMLP ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.HessMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for the HessMLP Class — summary.HessMLP","text":"summary object HessMLP object passed","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.HessMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Method for the HessMLP Class — summary.HessMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try HessianMLP sens <- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE) summary(sens) #> Hessian matrix of 2-5-1 MLP network. #>  #> Hessian measures of each output: #> $.outcome #> $.outcome$mean #>               WD        TEMP #> WD   -0.75371769 -0.02008632 #> TEMP -0.02008632  1.18930246 #>  #> $.outcome$std #>            WD     TEMP #> WD   2.414398 1.868676 #> TEMP 1.868676 6.269440 #>  #> $.outcome$meanSensSQ #>            WD     TEMP #> WD   2.528728 1.868313 #> TEMP 1.868313 6.379692 #>  #>"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for the SensMLP Class — summary.SensMLP","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"Print sensitivity metrics SensMLP object. metrics mean sensitivity, standard deviation sensitivities mean sensitivities square","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"","code":"# S3 method for SensMLP summary(object, ...)"},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"object SensMLP object created SensAnalysisMLP ... additional parameters","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"summary object SensMLP object passed","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"Pizarroso J, Portela J, Muñoz (2022). NeuralSens: Sensitivity Analysis Neural Networks. Journal Statistical Software, 102(7), 1-36.","code":""},{"path":"https://jaipizgon.github.io/NeuralSens/reference/summary.SensMLP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Method for the SensMLP Class — summary.SensMLP","text":"","code":"## Load data ------------------------------------------------------------------- data(\"DAILY_DEMAND_TR\") fdata <- DAILY_DEMAND_TR  ## Parameters of the NNET ------------------------------------------------------ hidden_neurons <- 5 iters <- 250 decay <- 0.1  ################################################################################ #########################  REGRESSION NNET ##################################### ################################################################################ ## Regression dataframe -------------------------------------------------------- # Scale the data fdata.Reg.tr <- fdata[,2:ncol(fdata)] fdata.Reg.tr[,3] <- fdata.Reg.tr[,3]/10 fdata.Reg.tr[,1] <- fdata.Reg.tr[,1]/1000  # Normalize the data for some models preProc <- caret::preProcess(fdata.Reg.tr, method = c(\"center\",\"scale\")) nntrData <- predict(preProc, fdata.Reg.tr)  #' ## TRAIN nnet NNET -------------------------------------------------------- # Create a formula to train NNET form <- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = \" + \") form <- formula(paste(names(fdata.Reg.tr)[1], form, sep = \" ~ \"))  set.seed(150) nnetmod <- nnet::nnet(form,                            data = nntrData,                            linear.output = TRUE,                            size = hidden_neurons,                            decay = decay,                            maxit = iters) #> # weights:  21 #> initial  value 2487.870002  #> iter  10 value 1587.516208 #> iter  20 value 1349.706741 #> iter  30 value 1333.940734 #> iter  40 value 1329.097060 #> iter  50 value 1326.518168 #> iter  60 value 1323.148574 #> iter  70 value 1322.378769 #> iter  80 value 1322.018091 #> final  value 1321.996301  #> converged # Try SensAnalysisMLP sens <- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE) summary(sens) #> Sensitivity analysis of 2-5-1 MLP network. #>  #> Sensitivity measures of each output: #> $.outcome #>           mean      std meanSensSQ #> WD    2.709330 1.182700   2.956103 #> TEMP -1.520092 4.172399   4.439684 #>"}]
